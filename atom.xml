<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>几米之家</title>
  
  <subtitle>Hold on to what we are, Hold on to your heart!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.hcrlp.com/"/>
  <updated>2018-12-29T09:24:16.805Z</updated>
  <id>http://www.hcrlp.com/</id>
  
  <author>
    <name>Jimmy Hua</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Neural Style Transfer</title>
    <link href="http://www.hcrlp.com/2018/12/29/Neural-Style-Transfer/"/>
    <id>http://www.hcrlp.com/2018/12/29/Neural-Style-Transfer/</id>
    <published>2018-12-29T09:23:13.000Z</published>
    <updated>2018-12-29T09:24:16.805Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Style Transfer 是一种通过CNN将一幅图像的内容与另一幅图像的风格相结合的算法.对于给定的一幅内容图像和一幅风格图像,算法的目标是生成一幅新的图像使得其与内容图像之间的内容差异最小,同时和风格图像之间的风格差异最小.</p><p>参考论文：</p><ul><li><a href="https://arxiv.org/abs/1508.06576" target="_blank" rel="noopener">A Neural Algorithm of Artistic Style</a></li><li><a href="https://arxiv.org/pdf/1603.08155.pdf" target="_blank" rel="noopener">Perceptual Losses for Real-Time Style Transfer<br>and Super-Resolution</a></li></ul><p align="center"><img width="100%" src="http://libai.91iot.net/hkLWpusQ"></p><h3 id="Principle"><a href="#Principle" class="headerlink" title="Principle"></a>Principle</h3><p>利用卷积网络提取图像特征，构造出风格迁移的基本原理：</p><blockquote><ol><li>两张图像经过预训练好的分类网络，若提取出的高维特征(high−levelhigh−level)之间的欧氏距离越小，则这两张图像内容越相似</li><li>两张图像经过与训练好的分类网络，若提取出的低维特征(low−levellow−level)在数值上基本相等，则这两张图像越相似，换句话说，两张图像相似等价于二者特征的GramGram矩阵具有较小的弗罗贝尼乌斯范数。</li></ol></blockquote><h4 id="Gram-Matrix"><a href="#Gram-Matrix" class="headerlink" title="Gram Matrix"></a>Gram Matrix</h4><ul><li><p>GramGram矩阵的数学形式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">G(x) = A*A^T</span><br></pre></td></tr></table></figure></li><li><p>Gram矩阵实际上是矩阵的内积运算，在风格迁移算法中，其计算的是feature map之间的偏心协方差，在feature map 包含着图像的特征，每个数字表示特征的强度，Gram矩阵代表着特征之间的相关性，因此，Gram矩阵可以用来表示图像的风格，因此可以通过Gram矩阵衡量风格的差异性。</p></li></ul><h4 id="Content-loss"><a href="#Content-loss" class="headerlink" title="Content loss"></a>Content loss</h4><p>内容差异最小化–首先将内容图像和目标图像分别送入预训练模型<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGGNet</a>,以提取特征.然后,通过最小化两个特征图之间的均方误差<a href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/neural_style_transfer/main.py#L81-L82" target="_blank" rel="noopener">mean-squared error</a>来更新目标图像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">content_loss += torch.mean((target_features - content_features) ** 2)</span><br></pre></td></tr></table></figure><h4 id="Style-loss"><a href="#Style-loss" class="headerlink" title="Style loss"></a>Style loss</h4><p>当计算content loss时,首先我们将风格图像和目标图像分别送入VGGNet来提取特征.为了生成纹理来匹配风格图像的风格,我们通过最小化协方差来更新目标图像.Style loss的计算方法可以看<a href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/neural_style_transfer/main.py#L84-L94" target="_blank" rel="noopener">论文</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># f1,f2,f3 &gt; target_features, content_features, style_features</span><br><span class="line"># 计算gram矩阵</span><br><span class="line">f1 = torch.mm(f1, f1.t())   # f1乘以f1的转置</span><br><span class="line">f3 = torch.mm(f3, f3.t())</span><br><span class="line"></span><br><span class="line"># 利用目标图像和风格图像计算风格损失</span><br><span class="line">style_loss += torch.mean((f1 - f3) ** 2) / (c * h * w)</span><br></pre></td></tr></table></figure><h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><p>使用训练好的 vgg19 网络来提取特征图，选用 conv1_1 ~ conv5_1作为特征图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGGNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""选择conv1_1 ~ conv5_1作为特征图."""</span></span><br><span class="line">        super(VGGNet, self).__init__()</span><br><span class="line">        self.select = [<span class="string">'0'</span>, <span class="string">'5'</span>, <span class="string">'10'</span>, <span class="string">'19'</span>, <span class="string">'28'</span>]</span><br><span class="line">        self.vgg = models.vgg19(pretrained=<span class="keyword">True</span>).features    <span class="comment"># vgg的到定义,可以作为pytorch如何实现finetune的参考(https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""提取特征图"""</span></span><br><span class="line">        features = []</span><br><span class="line">        <span class="keyword">for</span> name, layer <span class="keyword">in</span> self.vgg._modules.items():</span><br><span class="line">            x = layer(x)</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.select:</span><br><span class="line">                features.append(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure><h4 id="Load-image"><a href="#Load-image" class="headerlink" title="Load image"></a>Load image</h4><p>导入图片，resize 到指定的大小，并且转化为torch 格式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(image_path, transform=None, max_size=None, shape=None)</span>:</span></span><br><span class="line">    <span class="string">"""加载图像并将其转化为一个tensor."""</span></span><br><span class="line">    image = Image.open(image_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> max_size:</span><br><span class="line">        scale = max_size / max(image.size)</span><br><span class="line">        size = np.array(image.size) * scale</span><br><span class="line">        image = image.resize(size.astype(int), Image.ANTIALIAS)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shape:</span><br><span class="line">        image = image.resize(shape, Image.LANCZOS)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> transform:</span><br><span class="line">        image = transform(image).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image.to(device)</span><br></pre></td></tr></table></figure></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>风格转换：</p><p><img src="http://libai.91iot.net/OzfDvuN3" alt="image"></p><hr><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设备设置</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(image_path, transform=None, max_size=None, shape=None)</span>:</span></span><br><span class="line">    <span class="string">"""加载图像并将其转化为一个tensor."""</span></span><br><span class="line">    image = Image.open(image_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> max_size:</span><br><span class="line">        scale = max_size / max(image.size)</span><br><span class="line">        size = np.array(image.size) * scale</span><br><span class="line">        image = image.resize(size.astype(int), Image.ANTIALIAS)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shape:</span><br><span class="line">        image = image.resize(shape, Image.LANCZOS)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> transform:</span><br><span class="line">        image = transform(image).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGGNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""选择conv1_1 ~ conv5_1作为特征图."""</span></span><br><span class="line">        super(VGGNet, self).__init__()</span><br><span class="line">        self.select = [<span class="string">'0'</span>, <span class="string">'5'</span>, <span class="string">'10'</span>, <span class="string">'19'</span>, <span class="string">'28'</span>]</span><br><span class="line">        self.vgg = models.vgg19(pretrained=<span class="keyword">True</span>).features    <span class="comment"># vgg的到定义,可以作为pytorch如何实现finetune的参考(https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""提取特征图"""</span></span><br><span class="line">        features = []</span><br><span class="line">        <span class="keyword">for</span> name, layer <span class="keyword">in</span> self.vgg._modules.items():</span><br><span class="line">            x = layer(x)</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.select:</span><br><span class="line">                features.append(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(config)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 图像预处理</span></span><br><span class="line">    <span class="comment"># VGGNet预训练模型是基于ImageNet完成,其中图像经过了mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]的归一化</span></span><br><span class="line">    <span class="comment"># 在该项目中用相同的归一化操作</span></span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=(<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>),</span><br><span class="line">                             std=(<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载内容图像和风格图像</span></span><br><span class="line">    <span class="comment"># 使两个图像的大小相同</span></span><br><span class="line">    content = load_image(config.content, transform, max_size=config.max_size)</span><br><span class="line">    style = load_image(config.style, transform, shape=[content.size(<span class="number">2</span>), content.size(<span class="number">3</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将内容图像初始化为一幅目标图像</span></span><br><span class="line">    target = content.clone().requires_grad_(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[<span class="number">0.5</span>, <span class="number">0.999</span>])</span><br><span class="line">    vgg = VGGNet().to(device).eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(config.total_step):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取特征图</span></span><br><span class="line">        target_features = vgg(target)</span><br><span class="line">        content_features = vgg(content)</span><br><span class="line">        style_features = vgg(style)</span><br><span class="line"></span><br><span class="line">        style_loss = <span class="number">0</span></span><br><span class="line">        content_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> f1, f2, f3 <span class="keyword">in</span> zip(target_features, content_features, style_features):</span><br><span class="line">            <span class="comment"># 利用目标图像和内容图像计算content loss</span></span><br><span class="line">            content_loss += torch.mean((f1 - f2) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># reshape特征图</span></span><br><span class="line">            _, c, h, w = f1.size()</span><br><span class="line">            f1 = f1.view(c, h * w)</span><br><span class="line">            f3 = f3.view(c, h * w)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算gram矩阵</span></span><br><span class="line">            f1 = torch.mm(f1, f1.t())   <span class="comment"># f1乘以f1的转置</span></span><br><span class="line">            f3 = torch.mm(f3, f3.t())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 利用目标图像和风格图像计算风格损失</span></span><br><span class="line">            style_loss += torch.mean((f1 - f3) ** <span class="number">2</span>) / (c * h * w)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算总损失,反向传播即优化</span></span><br><span class="line">        loss = content_loss + config.style_weight * style_loss</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % config.log_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Step [&#123;&#125;/&#123;&#125;], Content Loss: &#123;:.4f&#125;, Style Loss: &#123;:.4f&#125;'</span></span><br><span class="line">                  .format(step + <span class="number">1</span>, config.total_step, content_loss.item(), style_loss.item()))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % config.sample_step == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 保存生成图像</span></span><br><span class="line">            denorm = transforms.Normalize((<span class="number">-2.12</span>, <span class="number">-2.04</span>, <span class="number">-1.80</span>), (<span class="number">4.37</span>, <span class="number">4.46</span>, <span class="number">4.44</span>))</span><br><span class="line">            img = target.clone().squeeze()</span><br><span class="line">            img = denorm(img).clamp_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            torchvision.utils.save_image(img, <span class="string">'output-&#123;&#125;.png'</span>.format(step + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">'neural style config'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--content'</span>, type=str, default=<span class="string">'png/lily.jpg'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--style'</span>, type=str, default=<span class="string">'png/style5.jpg'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--max_size'</span>, type=int, default=<span class="number">400</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--total_step'</span>, type=int, default=<span class="number">2000</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--log_step'</span>, type=int, default=<span class="number">10</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--sample_step'</span>, type=int, default=<span class="number">500</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--style_weight'</span>, type=float, default=<span class="number">100</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--lr'</span>, type=float, default=<span class="number">0.003</span>)</span><br><span class="line">    config = parser.parse_args()</span><br><span class="line">    print(config)</span><br><span class="line">    main(config)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;Style Transfer 是一种通过CNN将一幅图像的内
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://www.hcrlp.com/categories/Deep-Learning/"/>
    
    
      <category term="Pytorch" scheme="http://www.hcrlp.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Spatial Transformer Network</title>
    <link href="http://www.hcrlp.com/2018/12/26/Spatial-Transformer-Network/"/>
    <id>http://www.hcrlp.com/2018/12/26/Spatial-Transformer-Network/</id>
    <published>2018-12-26T09:35:08.000Z</published>
    <updated>2018-12-27T02:21:08.849Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Spatial transformer networks are a generalization of differentiable attention to any spatial transformation. Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model. For example, it can crop a region of interest, scale and correct the orientation of an image. It can be a useful mechanism because CNNs are not invariant to rotation and scale and more general affine transformations.</p><p><img src="https://i.loli.net/2018/12/26/5c233ca7b417f.png" width="500" div="" align="center"></p><p>STN has been learning how to deal with the images classification by amplifyinmg and eliminating background noise,and then get the standard input to improve the efficiency of the classification.</p><p>You could get more imformation via the <a href="https://goo.gl/qdEhUu." target="_blank" rel="noopener"><strong>Cartoon</strong></a></p><h3 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h3><p>Spatial transformer networks boils down to three main components :</p><ul><li>The localization network is a regular CNN which regresses the transformation parameters. The transformation is never learned explicitly from this dataset, instead the network learns automatically the spatial transformations that enhances the global accuracy.</li><li>The grid generator generates a grid of coordinates in the input image corresponding to each pixel from the output image.</li><li>The sampler uses the parameters of the transformation and applies it to the input image.<br><img src="http://libai.91iot.net/DFRrk3bK" alt="image"></li></ul><h4 id="Localisation-Network"><a href="#Localisation-Network" class="headerlink" title="Localisation Network"></a>Localisation Network</h4><p>The localisation network take the input feature map <strong>(W,H,C)</strong>, and retrun the outputs <strong>θ</strong> (spatial transformer parameters). We can get the parameters no matter fully connection or convolutional network.</p><h4 id="Grid-Generator"><a href="#Grid-Generator" class="headerlink" title="Grid Generator"></a>Grid Generator</h4><p>As we got the  <strong>θ</strong> parameters via the Localisation Net, and then using the affine transformation to transfer the pixel position from A to B.</p><p>The affine formula is as below:<br><img src="https://i.loli.net/2018/12/26/5c2343efed9e9.png"></p><p>The result of the affine transformation is as below:<br><img src="https://i.loli.net/2018/12/26/5c2343f083430.png"></p><h3 id="MNIST-Transformation"><a href="#MNIST-Transformation" class="headerlink" title="MNIST Transformation"></a>MNIST Transformation</h3><p>Now, we will try to work with the Mnist dataset to learn how to augment our network using a visual attention mechanism called spatial transformer networks.</p><h4 id="Data-load"><a href="#Data-load" class="headerlink" title="Data load"></a>Data load</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_availabe() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#traning data</span></span><br><span class="line">train_loader = torch.utils.data.Dataloader(</span><br><span class="line">    datasets.MNIST(root=<span class="string">'.'</span>,train=<span class="keyword">True</span>,download=<span class="keyword">True</span>,</span><br><span class="line">    transform=transforms.Compose([</span><br><span class="line">    transforms.ToTensor,</span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>,),(<span class="number">0.3081</span>,))])),</span><br><span class="line">    batch_size = <span class="number">64</span>, shuffle=<span class="keyword">True</span>,num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test dataset</span></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    datasets.MNIST(root=<span class="string">'.'</span>, train=<span class="keyword">False</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">    ])), batch_size=<span class="number">64</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h4 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h4><ul><li>Note: We need the latest version of PyTorch that contains <strong>affine_grid</strong> and <strong>grid_sample</strong> modules.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Spatial transformer localization-network</span></span><br><span class="line">        self.localization = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">8</span>, kernel_size=<span class="number">7</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Regressor for the 3 * 2 affine matrix</span></span><br><span class="line">        self.fc_loc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">32</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">32</span>, <span class="number">3</span> * <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the weights/bias with identity transformation</span></span><br><span class="line">        self.fc_loc[<span class="number">2</span>].weight.data.zero_()</span><br><span class="line">        self.fc_loc[<span class="number">2</span>].bias.data.copy_(torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=torch.float))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spatial transformer network forward function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">stn</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        xs = self.localization(x)</span><br><span class="line">        xs = xs.view(<span class="number">-1</span>, <span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>)</span><br><span class="line">        theta = self.fc_loc(xs)</span><br><span class="line">        theta = theta.view(<span class="number">-1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        grid = F.affine_grid(theta, x.size())</span><br><span class="line">        x = F.grid_sample(x, grid)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># transform the input</span></span><br><span class="line">        x = self.stn(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform the usual forward pass</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">320</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net().to(device)</span><br></pre></td></tr></table></figure><h4 id="Training-Model"><a href="#Training-Model" class="headerlink" title="Training Model"></a>Training Model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch)</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;'</span>.format(</span><br><span class="line">                epoch, batch_idx * len(data), len(train_loader.dataset),</span><br><span class="line">                <span class="number">100.</span> * batch_idx / len(train_loader), loss.item()))</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># A simple test procedure to measure STN the performances on MNIST.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        model.eval()</span><br><span class="line">        test_loss = <span class="number">0</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># sum up batch loss</span></span><br><span class="line">            test_loss += F.nll_loss(output, target, size_average=<span class="keyword">False</span>).item()</span><br><span class="line">            <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">            pred = output.max(<span class="number">1</span>, keepdim=<span class="keyword">True</span>)[<span class="number">1</span>]</span><br><span class="line">            correct += pred.eq(target.view_as(pred)).sum().item()</span><br><span class="line"></span><br><span class="line">        test_loss /= len(test_loader.dataset)</span><br><span class="line">        print(<span class="string">'\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n'</span></span><br><span class="line">              .format(test_loss, correct, len(test_loader.dataset),</span><br><span class="line">                      <span class="number">100.</span> * correct / len(test_loader.dataset)))</span><br></pre></td></tr></table></figure><h4 id="Visualizing-The-STN-Results"><a href="#Visualizing-The-STN-Results" class="headerlink" title="Visualizing The STN Results"></a>Visualizing The STN Results</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_image_np</span><span class="params">(inp)</span>:</span></span><br><span class="line">    <span class="string">"""Convert a Tensor to numpy image."""</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> inp</span><br><span class="line"></span><br><span class="line"><span class="comment"># We want to visualize the output of the spatial transformers layer</span></span><br><span class="line"><span class="comment"># after the training, we visualize a batch of input images and</span></span><br><span class="line"><span class="comment"># the corresponding transformed batch using STN.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_stn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># Get a batch of training data</span></span><br><span class="line">        data = next(iter(test_loader))[<span class="number">0</span>].to(device)</span><br><span class="line"></span><br><span class="line">        input_tensor = data.cpu()</span><br><span class="line">        transformed_input_tensor = model.stn(data).cpu()</span><br><span class="line"></span><br><span class="line">        in_grid = convert_image_np(</span><br><span class="line">            torchvision.utils.make_grid(input_tensor))</span><br><span class="line"></span><br><span class="line">        out_grid = convert_image_np(</span><br><span class="line">            torchvision.utils.make_grid(transformed_input_tensor))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Plot the results side-by-side</span></span><br><span class="line">        f, axarr = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        axarr[<span class="number">0</span>].imshow(in_grid)</span><br><span class="line">        axarr[<span class="number">0</span>].set_title(<span class="string">'Dataset Images'</span>)</span><br><span class="line"></span><br><span class="line">        axarr[<span class="number">1</span>].imshow(out_grid)</span><br><span class="line">        axarr[<span class="number">1</span>].set_title(<span class="string">'Transformed Images'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20</span> + <span class="number">1</span>):</span><br><span class="line">    train(epoch)</span><br><span class="line">    test()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the STN transformation on some input batch</span></span><br><span class="line">visualize_stn()</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>The results：<br><img src="https://i.loli.net/2018/12/26/5c233ca762f16.png" width="500" div="" align="center"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;Spatial transformer networks are a generalizat
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://www.hcrlp.com/categories/Deep-Learning/"/>
    
    
      <category term="Pytorch" scheme="http://www.hcrlp.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch-迁移学习</title>
    <link href="http://www.hcrlp.com/2018/12/25/Pytorch-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <id>http://www.hcrlp.com/2018/12/25/Pytorch-迁移学习/</id>
    <published>2018-12-25T02:48:44.000Z</published>
    <updated>2018-12-25T02:50:00.824Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h5><p>In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows:</p><ul><li style="list-style: none"><input type="checkbox" checked> <strong>ConvNet as fixed feature extractor</strong></li><li>Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset.</li><li style="list-style: none"><input type="checkbox" checked> <strong>Fine-tuning the ConvNet</strong></li><li>Instead of random initializaion, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.</li><li style="list-style: none"><input type="checkbox" checked> <strong>Pretrained models</strong></li></ul><h5 id="Sample"><a href="#Sample" class="headerlink" title="Sample"></a>Sample</h5><p>Today we use the transfer learning to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well.</p><p>This dataset is a very small subset of imagenet.</p><h6 id="Data-Load"><a href="#Data-Load" class="headerlink" title="Data Load"></a>Data Load</h6><p>Download the data from <a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip" target="_blank" rel="noopener">here</a></p><h6 id="Import-the-tools"><a href="#Import-the-tools" class="headerlink" title="Import the tools"></a>Import the tools</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure><h6 id="Data-Augment-and-Normalization-for-training"><a href="#Data-Augment-and-Normalization-for-training" class="headerlink" title="Data Augment and Normalization for training"></a>Data Augment and Normalization for training</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#just normalization for validation</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">'train'</span>:transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>,<span class="number">0.456</span>,<span class="number">0.406</span>],[<span class="number">0.228</span>,<span class="number">0.224</span>,<span class="number">0.225</span>])</span><br><span class="line">        ]),</span><br><span class="line">    <span class="string">'val'</span>:transforms.Compose([</span><br><span class="line">    transforms.Reszie(<span class="number">256</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>,<span class="number">0.456</span>,<span class="number">0.406</span>],[<span class="number">0.228</span>,<span class="number">0.224</span>,<span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">'data/hymenoptera_data'</span></span><br><span class="line">image_datasets = &#123;x:datasets.ImageFolder(os.path.join(data_dir,x),data_transforms[x])</span><br><span class="line">                  <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>,<span class="string">'val'</span>]&#125;</span><br><span class="line"></span><br><span class="line">dataloders = &#123;x:torch.utils.data.Dataloader(image_datasets[x],bath_szie=<span class="number">4</span>,</span><br><span class="line">                                            shuffle=<span class="keyword">True</span>,num_works=<span class="number">4</span>)</span><br><span class="line">              <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>,<span class="string">'val'</span>]&#125;</span><br><span class="line"></span><br><span class="line">dataset_sizes = &#123;x:len(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>,<span class="string">'val'</span>]&#125;</span><br><span class="line">class_names = image_datasets[<span class="string">'train'</span>].classes</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><h6 id="Visualize-a-few-images"><a href="#Visualize-a-few-images" class="headerlink" title="Visualize a few images"></a>Visualize a few images</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(inp, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch of training data</span></span><br><span class="line">inputs, classes = next(iter(dataloaders[<span class="string">'train'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a grid from batch</span></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line">imshow(out, title=[class_names[x] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2018/12/24/5c20977ee7f85.png"></p><h6 id="Training-Model"><a href="#Training-Model" class="headerlink" title="Training Model"></a>Training Model</h6><p>Now, let’s write a general function to train a model. Here, we will illustrate:</p><p>Scheduling the learning rate<br>Saving the best model<br>In the following, parameter ==scheduler== is an LR scheduler object from ==torch.optim.lr_scheduler==.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line">                model.train()     <span class="comment">#Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.eval()</span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># zero the parameter</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="string">'''forward'''</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">'train'</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _,preds = torch.max(outputs,<span class="number">1</span>)</span><br><span class="line">                    loss = criterion(outputs,labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment">#statistics</span></span><br><span class="line">                running_loss += loss.item()*inputs.size()[<span class="number">0</span>]</span><br><span class="line">                running_correct += torch.sum(preds == labels.data)</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># deep copy the model</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load best model weights</span></span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h6 id="Visualizing-the-model-predictions"><a href="#Visualizing-the-model-predictions" class="headerlink" title="Visualizing the model predictions"></a>Visualizing the model predictions</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_model</span><span class="params">(model, num_images=<span class="number">6</span>)</span>:</span></span><br><span class="line">    was_training = model.training</span><br><span class="line">    model.eval()</span><br><span class="line">    images_so_far = <span class="number">0</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(dataloaders[<span class="string">'val'</span>]):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(inputs.size()[<span class="number">0</span>]):</span><br><span class="line">                images_so_far += <span class="number">1</span></span><br><span class="line">                ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</span><br><span class="line">                ax.axis(<span class="string">'off'</span>)</span><br><span class="line">                ax.set_title(<span class="string">'predicted: &#123;&#125;'</span>.format(class_names[preds[j]]))</span><br><span class="line">                imshow(inputs.cpu().data[j])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">                    model.train(mode=was_training)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">        model.train(mode=was_training)</span><br></pre></td></tr></table></figure><h6 id="Finetuning-the-ConvNet"><a href="#Finetuning-the-ConvNet" class="headerlink" title="Finetuning the ConvNet"></a>Finetuning the ConvNet</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=True)</span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line">model_fc.fc = nn.Linear(num_ftrs,2)</span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"># Observe that all parameters are being optimized</span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)</span><br><span class="line"></span><br><span class="line"># Decay LR by a factor of 0.1 every 7 epochs</span><br><span class="line"># torch.optim.lr_scheduler模块的StepLR类，表示每隔step_size个epoch就将学习率降为原来的gamma倍。</span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)</span><br></pre></td></tr></table></figure><h6 id="Train-and-evaluate"><a href="#Train-and-evaluate" class="headerlink" title="Train and evaluate"></a>Train and evaluate</h6><p>It should take around 15-25 min on CPU. On GPU though, it takes less than a minute.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize_model(model_ft)</span><br></pre></td></tr></table></figure><p><img src="https://pytorch.org/tutorials/_images/sphx_glr_transfer_learning_tutorial_002.png" alt="image"></p><h5 id="Convnet-as-fixed-feature-extractor"><a href="#Convnet-as-fixed-feature-extractor" class="headerlink" title="Convnet as fixed feature extractor"></a>Convnet as fixed feature extractor</h5><p>Here, we need to freeze all the network except the final layer. We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward().</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that only parameters of final layer are being optimized as</span></span><br><span class="line"><span class="comment"># opoosed to before.</span></span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>train the model:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure></p><p>show the image:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">visualize_model(model_conv)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="https://pytorch.org/tutorials/_images/sphx_glr_transfer_learning_tutorial_003.png" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;Introduce&quot;&gt;&lt;a href=&quot;#Introduce&quot; class=&quot;headerlink&quot; title=&quot;Introduce&quot;&gt;&lt;/a&gt;Introduce&lt;/h5&gt;&lt;p&gt;In practice, very few people train an enti
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://www.hcrlp.com/categories/Deep-Learning/"/>
    
    
      <category term="Pytorch" scheme="http://www.hcrlp.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>The Learning Strategy of the CS231n</title>
    <link href="http://www.hcrlp.com/2018/12/14/The-Learning-Strategy-of-the-CS231n/"/>
    <id>http://www.hcrlp.com/2018/12/14/The-Learning-Strategy-of-the-CS231n/</id>
    <published>2018-12-14T07:05:43.000Z</published>
    <updated>2018-12-17T06:17:17.927Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h3><p>对于算法工程师，不同的人的认知角度都是不同的，我们通过下面三个知乎的高票回答帮助大家了解算法工程师到底需要做什么样的事，工业界需要什么样的能力<br>链接：<br>从今年校招来看，机器学习等算法岗位应届生超多，竞争激烈，未来 3-5 年机器学习相关就业会达到饱和吗？ - Cheeeen的回答 - 知乎<br> <a href="https://www.zhihu.com/question/66406672/answer/317489657" target="_blank" rel="noopener">https://www.zhihu.com/question/66406672/answer/317489657</a></p><p>2019 秋招的 AI 岗位竞争激烈吗？ - 王剑锋的回答 - 知乎<br> <a href="https://www.zhihu.com/question/286925266/answer/491117602" target="_blank" rel="noopener">https://www.zhihu.com/question/286925266/answer/491117602</a></p><p>论算法工程师首先是个工程师之深度学习在排序应用踩坑总结 - 吴海波的文章 - 知乎<br> <a href="https://zhuanlan.zhihu.com/p/44315278" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44315278</a></p><h3 id="二、时间安排"><a href="#二、时间安排" class="headerlink" title="二、时间安排"></a>二、时间安排</h3><p>每周具体学习时间划分为4个部分:</p><blockquote><ol><li>部分安排周一到周二</li><li>部分安排在周四到周五</li><li>部分安排在周日</li><li>部分作业是是任何有空的时间自行完成，可以落后于学习进度</li><li>周三、周六休息</li></ol></blockquote><h3 id="三、课程资料"><a href="#三、课程资料" class="headerlink" title="三、课程资料"></a>三、课程资料</h3><p>课程主页： <a href="http://cs231n.stanford.edu" target="_blank" rel="noopener">http://cs231n.stanford.edu</a><br>英文笔记： <a href="http://cs231n.github.io" target="_blank" rel="noopener">http://cs231n.github.io</a><br>中文笔记： <a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21930884</a><br>课程视频： <a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>环境配置： <a href="https://github.com/sharedeeply/DeepLearning-StartKit" target="_blank" rel="noopener">https://github.com/sharedeeply/DeepLearning-StartKit</a><br>作业链接： [作业] <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1" target="_blank" rel="noopener">https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1</a><br>作业参考： <a href="https://github.com/sharedeeply/cs231n-assignment-solution" target="_blank" rel="noopener">https://github.com/sharedeeply/cs231n-assignment-solution</a><br>课程课件： <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/slides" target="_blank" rel="noopener">https://github.com/sharedeeply/cs231n-camp/tree/master/slides</a></p><p>注册一个github账号：github.com<br>后续发布的一些project和exercise会在这个github下：<br> <a href="https://sharedeeply.github.io/cs231n-camp" target="_blank" rel="noopener">https://sharedeeply.github.io/cs231n-camp</a></p><h3 id="四、知识工具"><a href="#四、知识工具" class="headerlink" title="四、知识工具"></a>四、知识工具</h3><h4 id="数学工具"><a href="#数学工具" class="headerlink" title="数学工具"></a>数学工具</h4><p>cs229资料：<br>线性代数：<a href="http://web.stanford.edu/class/cs224n/readings/cs229-linalg.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/readings/cs229-linalg.pdf</a><br>概率论：<a href="http://101.96.10.44/web.stanford.edu/class/cs224n/readings/cs229-prob.pdf" target="_blank" rel="noopener">http://101.96.10.44/web.stanford.edu/class/cs224n/readings/cs229-prob.pdf</a><br>凸函数优化：<a href="http://101.96.10.43/web.stanford.edu/class/cs224n/readings/cs229-cvxopt.pdf" target="_blank" rel="noopener">http://101.96.10.43/web.stanford.edu/class/cs224n/readings/cs229-cvxopt.pdf</a><br>随机梯度下降算法：<a href="http://cs231n.github.io/optimization-1" target="_blank" rel="noopener">http://cs231n.github.io/optimization-1</a><br>中文资料：<br>机器学习中的数学基本知识：<a href="https://www.cnblogs.com/steven-yang/p/6348112.html" target="_blank" rel="noopener">https://www.cnblogs.com/steven-yang/p/6348112.html</a><br>统计学习方法：<a href="http://vdisk.weibo.com/s/vfFpMc1YgPOr" target="_blank" rel="noopener">http://vdisk.weibo.com/s/vfFpMc1YgPOr</a><br>大学数学课本从故纸堆里翻出来</p><h4 id="编程工具"><a href="#编程工具" class="headerlink" title="编程工具"></a>编程工具</h4><p>Python复习：<a href="http://web.stanford.edu/class/cs224n/lectures/python-review.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/lectures/python-review.pdf</a><br>PyTorch教程： <a href="https://cn.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener">https://cn.udacity.com/course/deep-learning-pytorch--ud188</a><br>TensorFlow教程：<a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">https://github.com/aymericdamien/TensorFlow-Examples</a><br>廖雪峰python3教程：<a href="https://www.liaoxuefeng.com/article/001432619295115c918a094d8954bd493037b03d27bf9a9000" target="_blank" rel="noopener">廖雪峰python3</a><br>github教程：<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000</a><br>深度学习的学习路线：<a href="https://github.com/L1aoXingyu/Roadmap-of-DL-and-ML/blob/master/README_cn.md" target="_blank" rel="noopener">https://github.com/L1aoXingyu/Roadmap-of-DL-and-ML/blob/master/README_cn.md</a><br>开源深度学习课程：<a href="http://www.deeplearningweekly.com/blog/open-source-deep-learning-curriculum" target="_blank" rel="noopener">http://www.deeplearningweekly.com/blog/open-source-deep-learning-curriculum</a><br>mxnet/gluon教程： <a href="https://zh.gluon.ai" target="_blank" rel="noopener">https://zh.gluon.ai</a><br>知乎专栏：<a href="https://zhuanlan.zhihu.com/c_94953554" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/c_94953554</a><br>pytorch教程：<a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch" target="_blank" rel="noopener">https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch</a></p><h3 id="五、学习计划"><a href="#五、学习计划" class="headerlink" title="五、学习计划"></a>五、学习计划</h3><h4 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1:"></a>Week 1:</h4><ul><li><p>第1部分学习任务：<br>1听深度学习绪论+0基础一小时完成一场比赛PPT<br>学习时长：10/30<br>任务详解：视频1主要是介绍深度学习的一些应用案例，第二个是完成一次kaggle比赛的流程<br>2了解计算机视觉综述，历史背景和课程大纲<br>学习时长：10/30<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture01.pdf" target="_blank" rel="noopener">lecture01</a><br>观看视频 p1, p2 和 p3<br>课程视频： <a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a></p></li><li><p>第2部分学习任务：<br>1学习数据驱动的方法, 理解 KNN 算法，初步学习线性分类器<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture01.pdf" target="_blank" rel="noopener">lecture02</a><br>观看视频 p4 p5 和 p6：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习 图像分类笔记上：<a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit</a><br>学习 图像分类笔记下：<a href="https://zhuanlan.zhihu.com/p/20900216" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20900216</a><br>学习 线性分类笔记上：<a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit</a><br>学习时长：11/1—11/2</p></li><li><p>第3部分学习任务：<br>1掌握本门课 python 编程的基本功<br>阅读 python 和 numpy 教程：<a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit</a><br>代码：<a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit</a><br>学习时长：11/4</p></li><li><p>第4部分作业：(热身)写一个矩阵的类，实现矩阵乘法，只能使用 python 的类(class)和列表(list)<br>完成assignment1 中的 knn.ipynb<br>作业链接： <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1" target="_blank" rel="noopener">作业</a></p></li></ul><hr><h4 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2:"></a>Week 2:</h4><ul><li><p>第1部分学习任务：<br>1深入理解线性分类器的原理<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture03.pdf" target="_blank" rel="noopener">lecture03</a><br>观看视频 p7：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习 线性分类笔记中：<a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit</a><br>学习 线性分类笔记下：<a href="https://zhuanlan.zhihu.com/p/21102293" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21102293</a><br>学习时长：11/5—11/6</p></li><li><p>第2部分学习任务：<br>1学习损失函数以及梯度下降的相关知识<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture03.pdf" target="_blank" rel="noopener">lecture03</a><br>观看视频 p8：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习 最优化笔记：<a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit</a><br>学习时长：11/8—11/9</p></li><li><p>第3部分学习任务：<br>1掌握矩阵求导的基本方法<br>根据资料，学习矩阵求导的基本技巧，看多少内容取决于个人需要：<a href="https://zhuanlan.zhihu.com/p/25063314" target="_blank" rel="noopener">矩阵求导</a><br>学习时长：11/11</p></li><li><p>第4部分作业：<br>(1)简述 KNN 和线性分类器的优劣<br>(2)完成assignment1 中 svm.ipynb<br>作业链接： <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1" target="_blank" rel="noopener">作业</a></p></li></ul><hr><h4 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3:"></a>Week 3:</h4><ul><li><p>第1部分学习任务：<br>1学习掌握深度学习的基石: 反向传播算法<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture04.pdf" target="_blank" rel="noopener">lecture04</a><br>观看视频 p9：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习反向传播算法的笔记：<a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit</a><br>学习时长：11/12—11/13</p></li><li><p>第2部分学习任务：<br>1理解神经网络的结构和原理<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture04.pdf" target="_blank" rel="noopener">lecture04</a><br>观看视频 p10：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习时长：11/15—11/16</p></li><li><p>第3部分学习任务：<br>1深入理解反向传播算法<br>阅读反向传播算法的数学补充：<a href="http://cs231n.stanford.edu/handouts/derivatives.pdf" target="_blank" rel="noopener">http://cs231n.stanford.edu/handouts/derivatives.pdf</a><br>例子：<a href="http://cs231n.stanford.edu/handouts/linear-backprop.pdf" target="_blank" rel="noopener">http://cs231n.stanford.edu/handouts/linear-backprop.pdf</a><br>学习时长：11/18</p></li><li><p>第4部分作业：<br>完成 assignment1 中的 softmax.ipynb<br>完成 assignment1 中的 two_layer_net.ipynb<br>作业链接： <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1" target="_blank" rel="noopener">作业</a></p></li></ul><hr><h4 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4:"></a>Week 4:</h4><ul><li><p>第1部分学习任务：<br>1掌握 PyTorch 中的基本操作<br>学习 pytorch 的入门基础：<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</a><br>学习时长：11/19—11/20</p></li><li><p>第2部分学习任务：<br>1了解 kaggle 比赛的流程，并完成第一次的成绩提交<br>了解比赛房价预测：<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" target="_blank" rel="noopener">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</a><br>学习模板代码：<a href="https://github.com/L1aoXingyu/kaggle-house-price" target="_blank" rel="noopener">https://github.com/L1aoXingyu/kaggle-house-price</a><br>学习时长：11/22—11/23</p></li><li><p>第3部分学习任务：<br>1学习 PyTorch 中的数据读取<br>学习官方教程：<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a><br>学习中文笔记：<a href="https://l1aoxingyu.github.io/2017/10/23/PyTorch%E5%AE%9E%E7%8E%B0%E8%87%AA%E7%94%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96" target="_blank" rel="noopener">笔记</a></p></li></ul><p>学习时长：11/25</p><ul><li>第4部分作业：<br>1完成 assignment1 中的 features.ipynb<br>提交方式：发送到训练营公共邮箱<br>2修改房价预测的代码，在训练营里打卡提交 kaggle 的成绩<br>作业链接： <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1" target="_blank" rel="noopener">作业</a></li></ul><hr><h4 id="Week-5"><a href="#Week-5" class="headerlink" title="Week 5:"></a>Week 5:</h4><ul><li><p>第1部分学习任务：<br>1理解 CNN 中的卷积<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture05.pdf" target="_blank" rel="noopener">lecture05</a><br>观看视频 p11, p12：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习时长：11/26—11/27</p></li><li><p>第2部分学习任务：<br>1理解 CNN 中的 pooling<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture05.pdf" target="_blank" rel="noopener">lecture05</a><br>观看视频 p13：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习卷积神经网络笔记：<a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit</a><br>学习时长：11/29—11/30</p></li><li><p>第3部分学习任务：<br>1完成 CNN 的第一个应用练习，人脸关键点检测<br>阅读 facial keypoint 小项目：<a href="https://github.com/udacity/P1_Facial_Keypoints" target="_blank" rel="noopener">https://github.com/udacity/P1_Facial_Keypoints</a><br>参考代码：<a href="https://github.com/L1aoXingyu/P1_Facial_Keypoints" target="_blank" rel="noopener">https://github.com/L1aoXingyu/P1_Facial_Keypoints</a><br>学习时长：12/02</p></li><li><p>第4部分作业：<br>1完成 assignment2 中 FullyConnectedNets.ipynb<br>提交方式：发送到训练营公共邮箱<br>2思考一下卷积神经网络对比传统神经网络的优势在哪里？为什么更适合处理图像问题，在训练营里打卡提交<br>作业链接： <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1" target="_blank" rel="noopener">作业</a></p></li></ul><hr><h4 id="Week-6"><a href="#Week-6" class="headerlink" title="Week 6:"></a>Week 6:</h4><ul><li><p>第1部分学习任务：<br>1理解激活函数，权重初始化，batchnorm 对网络训练的影响<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture06.pdf" target="_blank" rel="noopener">lecture06</a><br>观看视频 p14：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习神经网络笔记1：<a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit</a><br>学习时长：12/03—12/04</p></li><li><p>第2部分学习任务：<br>1深入理解 BatchNormalization<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture06.pdf" target="_blank" rel="noopener">lecture06</a><br>观看视频 p15：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习神经网络笔记2：<a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit</a><br>学习时长：12/06—12/07</p></li><li><p>第3部分学习任务：<br>1总结回顾和理解深度学习中 normalize 的技巧<br>阅读文章 深度学习中的 normalization 方法：<a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33173246</a><br>学习时长：12/09</p></li><li><p>第4部分作业：<br>完成 assignment2 中 BatchNormalization.ipynb<br>完成 assignment2 中 Dropout.ipynb<br>提交方式：发送到训练营公共邮箱<br>作业链接： <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1" target="_blank" rel="noopener">作业</a></p></li></ul><hr><h4 id="Week-7"><a href="#Week-7" class="headerlink" title="Week 7:"></a>Week 7:</h4><ul><li><p>第1部分学习任务：<br>1理解更 fancy 的优化方法，更多的 normalize 以及正则化和迁移学习对网络训练的影响<br>slides: <a href="https://github.com/sharedeeply/cs231n-camp/blob/master/slides/cs231n_2018_lecture07.pdf" target="_blank" rel="noopener">lecture07</a><br>观看视频 p16，p17，p18：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">https://www.bilibili.com/video/av17204303</a><br>学习神经网络笔记3：<a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit</a><br>学习时长：12/10—12/11</p></li><li><p>第2部分学习任务：<br>1了解第二次的 kaggle 比赛 cifar10 分类<br>报名 cifar10 比赛：<a href="https://www.kaggle.com/c/cifar-10" target="_blank" rel="noopener">https://www.kaggle.com/c/cifar-10</a><br>学习模板代码：<a href="https://github.com/L1aoXingyu/kaggle-cifar10" target="_blank" rel="noopener">https://github.com/L1aoXingyu/kaggle-cifar10</a><br>学习时长：12/13—12/14</p></li><li><p>第4部分作业：<br>1完成 assignment2 中 ConvolutionNetworks.ipynb<br>提交方式：发送到训练营公共邮箱<br>2修改 cifar10 的网络结构，在训练营里打卡提交 kaggle 成绩<br>作业链接： <a href="https://github.com/sharedeeply/cs231n-camp/tree/master/assignment/assignment1" target="_blank" rel="noopener">作业</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h3&gt;&lt;p&gt;对于算法工程师，不同的人的认知角度都是不同的，我们通过下面三个知乎的高票回答帮助大家了解算法工程师到底需要做什么样的事，工业
      
    
    </summary>
    
      <category term="Computer Vision" scheme="http://www.hcrlp.com/categories/Computer-Vision/"/>
    
    
      <category term="cs231n" scheme="http://www.hcrlp.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>Yolo2 训练自己数据集</title>
    <link href="http://www.hcrlp.com/2018/12/10/Yolo2-%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://www.hcrlp.com/2018/12/10/Yolo2-训练自己数据集/</id>
    <published>2018-12-10T10:36:03.000Z</published>
    <updated>2018-12-17T06:15:08.663Z</updated>
    
    <content type="html"><![CDATA[<p>####1. 添加数据分类的各个类别名称文 data/rabbit.names:</p><p><a href="https://i.loli.net/2018/12/10/5c0e407334b25.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2018/12/10/5c0e407334b25.png" alt="1.png"></a></p><p>####2. 添加配置文件/cfg/rabbit.data，并且修改自己需要的类别和训练数据集：</p><p><a href="https://i.loli.net/2018/12/10/5c0e407341624.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2018/12/10/5c0e407341624.png" alt="2.png"></a></p><p>####3. 修改网络配置 /cfg/yolov2-tiny-voc.cfg:</p><p><a href="https://i.loli.net/2018/12/10/5c0e40733ea05.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2018/12/10/5c0e40733ea05.png" alt="3.png"></a></p><p><a href="https://i.loli.net/2018/12/10/5c0e40735f4a6.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2018/12/10/5c0e40735f4a6.png" alt="4.png"></a></p><p>####4. 修改 Makefile 改成GPU训练：</p><p><a href="https://i.loli.net/2018/12/10/5c0e4073649c9.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2018/12/10/5c0e4073649c9.png" alt="5.png"></a></p><p>####5. 运行命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* 训练：./darknet detector train cfg/rabbit.data cfg/yolov2-tiny.cfg darknet19_448.conv.23</span><br><span class="line"></span><br><span class="line">* 测试：./darknet detector <span class="built_in">test</span> cfg/rabbit.data cfg/yolov2-tiny.cfg backup/tiny-yolo-1/yolov2-tiny_50000.weights data/pic_1350.png</span><br><span class="line"></span><br><span class="line">* recall：./darknet detector recall cfg/rabbit.data cfg/yolov2-tiny.cfg backup/tiny-yolo-1/yolov2-tiny_50000.weights</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;####1. 添加数据分类的各个类别名称文 data/rabbit.names:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://i.loli.net/2018/12/10/5c0e407334b25.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
      
    
    </summary>
    
      <category term="Computer Vision" scheme="http://www.hcrlp.com/categories/Computer-Vision/"/>
    
    
      <category term="yolo2" scheme="http://www.hcrlp.com/tags/yolo2/"/>
    
  </entry>
  
  <entry>
    <title>语义分割-PSPNET</title>
    <link href="http://www.hcrlp.com/2018/11/29/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-PSPNET/"/>
    <id>http://www.hcrlp.com/2018/11/29/语义分割-PSPNET/</id>
    <published>2018-11-29T08:19:30.000Z</published>
    <updated>2018-12-19T07:53:42.292Z</updated>
    
    <content type="html"><![CDATA[<p><strong> PSPNET:</strong> <a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="noopener">Pyramid Scene Parsing Network</a></p><p><strong>From :</strong> CVPR 2017(IEEE Conference on Computer Vision and Pattern Recognition)</p><p><strong>代码：</strong></p><ul><li><p><a href="https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow" target="_blank" rel="noopener">PSPnet-Keras-TensorFlow</a></p></li><li><p><a href="https://github.com/Lextal/pspnet-pytorch" target="_blank" rel="noopener">PSPnet-Pytorch</a></p></li></ul><p><strong>效果：</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/11679057-cb0abf4f5eaf8f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp" width="700"></p><p><strong>对比传统方法：</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/11679057-59d0704d1924bf7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp" width="700"></p><hr><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文提出的金字塔池化模块( pyramid pooling module)能够<strong>聚合不同区域的上下文信息</strong>,从而提高获取全局信息的能力。实验表明这样的先验表示(即指代PSP这个结构)是有效的，在多个数据集上展现了优良的效果。</p><hr><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>场景解析(Scene Parsing)的难度与场景的标签密切相关。先大多数先进的场景解析框架大多数基于FCN，但FCN存在的几个问题：</p><ul><li>Mismatched Relationship ：上下文关系匹配对理解复杂场景很重要，FCN缺少上下文推断能力</li><li>Confusion categories ：许多标签之间存在关联，可以通过标签之间的关系弥补</li><li>Inconspicuous Classes ：模型可能会忽略小的东西，而大的东西又可能超过FCN的接受范围</li></ul><p>总结这些情况，许多问题出在FCN不能有效的处理场景之间的关系和全局信息。本论文提出了能够获取全局场景的深度网络PSPNet，能够融合合适的全局特征，将局部和全局信息融合到一起。并提出了一个适度监督损失的优化策略，在多个数据集上表现优异。</p><p>本文的主要贡献如下：</p><ol><li>提出了一个金字塔场景解析网络，能够将难解析的场景信息特征嵌入基于FCN预测框架中</li><li>在基于深度监督损失ResNet上制定有效的优化策略</li><li>构建了一个实用的系统，用于场景解析和语义分割，并包含了实施细节</li></ol><hr><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p>受到深度神经网络的驱动，场景解析和语义分割获得了极大的进展。例如FCN、ENet等工作。许多深度卷积神经网络为了扩大高层feature的感受野，常用dilated convolution(空洞卷积)、coarse-to-fine structure等方法。本文基于先前的工作，选择的baseline是带dilated network的FCN。</p><p>大多数语义分割模型的工作基于两个方面：</p><ol><li>具有多尺度的特征融合，高层特征具有强的语义信息，底层特征包含更多的细节。</li><li>基于结构预测。例如使用CRF(条件随机场)做后端细化分割结果。</li></ol><p>为了充分的利用全局特征层次先验知识来进行不同场景理解，本文提出的PSP模块能够聚合不同区域的上下文从而达到获取全局上下文的目的。</p><hr><h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><h5 id="1-Pyramid-Pooling-Module"><a href="#1-Pyramid-Pooling-Module" class="headerlink" title=" 1. Pyramid Pooling Module "></a><font color="#0099ff" size="4"> 1. Pyramid Pooling Module </font></h5><blockquote><p>前面也说到了，本文的一大贡献就是PSP模块。</p><p>在一般CNN中感受野可以粗略的认为是使用上下文信息的大小，论文指出在许多网络中没有充分的获取全局信息，所以效果不好。要解决这一问题，常用的方法是：</p><ul><li>用全局平均池化处理。但这在某些数据集上，可能会失去空间关系并导致模糊。</li><li>由金字塔池化产生不同层次的特征最后被平滑的连接成一个FC层做分类。这样可以去除CNN固定大小的图像分类约束，减少不同区域之间的信息损失。</li></ul><p>论文提出了一个具有层次全局优先级，包含不同子区域之间的不同尺度的信息，称之为pyramid pooling module。<br><img src="https://img-blog.csdn.net/20180331205402666?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podXplbWluNDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p>该模块融合了4种不同金字塔尺度的特征，第一行红色是最粗糙的特征–全局池化生成单个bin输出，后面三行是不同尺度的池化特征。为了保证全局特征的权重，如果金字塔共有N个级别，则在每个级别后使用1×1的卷积将对于级别通道降为原本的1/N。再通过双线性插值获得未池化前的大小，最终concat到一起。</p><p>金字塔等级的池化核大小是可以设定的，这与送到金字塔的输入有关。论文中使用的4个等级，核大小分别为1×1，2×2，3×3，6×6。</p></blockquote><p>示例代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PyramidPool</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,in_planes,out_planes,sizes=<span class="params">(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>)</span>)</span>:</span></span><br><span class="line">        super(PyramidPool,self).__init__()</span><br><span class="line">        self.pool_list = [nn.ModuleList([self._make_pool(in_planes,size) <span class="keyword">for</span> size <span class="keyword">in</span> sizes])]</span><br><span class="line">        self.bottleneck = nn.Conv2d((in_planes)*<span class="number">2</span>,out_planes,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.relu = nn.ReLU</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_pool</span><span class="params">(self,in_planes,size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                            nn.AdaptiveAvgPool2d(output_size=(size,size)),</span><br><span class="line">                            nn.Conv2d(in_planes,in_planes//<span class="number">4</span>,kernel_size=<span class="number">1</span>,bias=<span class="keyword">False</span>),</span><br><span class="line">                            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        priors = [F.upsample_bilinear(input=pool,size=x.size[<span class="number">2</span>:]) <span class="keyword">for</span> pool <span class="keyword">in</span> self.pool_list]+[x]</span><br><span class="line">        cat_priors = torch.cat(priors,<span class="number">1</span>)</span><br><span class="line">        bottle = self.bottleneck(cat_priors)</span><br><span class="line">        <span class="keyword">return</span> self.relu(bottle)</span><br></pre></td></tr></table></figure></p><h5 id="2-整体框架"><a href="#2-整体框架" class="headerlink" title="2. 整体框架"></a>2. 整体框架</h5><blockquote><ul><li>基础层经过预训练的模型(ResNet101)和空洞卷积策略提取feature map,提取后的feature map是输入的1/8大小</li><li>feature map经过Pyramid Pooling Module得到融合的带有整体信息的feature，在上采样与池化前的feature map相concat</li><li>最后过一个卷积层得到最终输出</li></ul></blockquote><h5 id="3-Auxiliary-Network"><a href="#3-Auxiliary-Network" class="headerlink" title="3. Auxiliary Network"></a>3. Auxiliary Network</h5><blockquote><p>在ResNet101的基础上做了改进，除了使用后面的softmax分类做loss，额外的在第四阶段添加了一个辅助的loss，两个loss一起传播，使用不同的权重，共同优化参数。后续的实验证明这样做有利于快速收敛</p></blockquote><p>PSPNet示例代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PSPNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_class,sizes=<span class="params">(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>)</span>,spp_size=<span class="number">2048</span>,pretrained=True)</span>:</span></span><br><span class="line">        super(PSPNet,self).__init__()</span><br><span class="line">        self.resnet = resnet.resnet50(pretrained)</span><br><span class="line">        self.spp = PyramidPool(spp_size,<span class="number">1024</span>,sizes)</span><br><span class="line">        <span class="comment">#主分支</span></span><br><span class="line">        self.final = nn.Sequential(</span><br><span class="line">                                nn.Conv2d(<span class="number">4096</span>,<span class="number">512</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>,dilation=<span class="number">1</span>),</span><br><span class="line">                                nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">                                nn.ReLU(),</span><br><span class="line">                                nn.Dropout2d(p=<span class="number">0.3</span>),</span><br><span class="line">                                nn.Conv2d(<span class="number">512</span>,num_class,<span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                                )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 辅助分支</span></span><br><span class="line">        self.aux = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">1024</span>,<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">256</span>,num_class),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="string">''' init weight '''</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m,nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight,mode=<span class="string">'fan_out'</span>,nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias,<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m,nn.BatchNorm2d):</span><br><span class="line">                nn.init.constant_(m.weight,<span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            aux,h = self.resnet(x)</span><br><span class="line">            aux_ = self.aux(aux)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            h = self.resnet(x)</span><br><span class="line">        h = spp(h)</span><br><span class="line">        h = self.final(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> aux_,h</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure></p><hr><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><p>论文在ImageNet scene parsing challenge 2016, PASCAL VOC 2012,Cityscapes 三个数据集上做了实验。</p><p>原作训练细节：</p><table><thead><tr><th>项目</th><th>设置</th></tr></thead><tbody><tr><td>学习率</td><td>采用“poly”策略，即lr=lrbase∗(1−itermaxiter)power 设置lrbase=0.01,power=0.9，衰减动量设置为0.9 and 0.0001</td></tr><tr><td>迭代次数</td><td>ImageNet上设置150K,PASCAL VOC设置30K，Cityscapes设置90K</td></tr><tr><td>数据增强</td><td>随机翻转、尺寸在0.5到2之间缩放、角度在-10到10之间旋转、随机的高斯滤波</td></tr><tr><td>batchsize</td><td>batch很重要，设置batch=16(这很吃显存啊~)</td></tr><tr><td>训练分支网络</td><td>设置辅助loss的权重为0.4</td></tr><tr><td>平台</td><td>Caffe</td></tr></tbody></table><hr><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>论文在结构上提供了一个pyramid pooling module，在不同层次上融合feature,达到语义和细节的融合。 模型的性能表现很大，但感觉主要归功于一个良好的特征提取层。在实验部分讲了很多训练细节，但还是很难复现，这里值得好好推敲一下。</p><hr><h4 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h4><p>Resnet50 示例代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">@Time:   2018-12-03 11:01:14</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: JimmyHua</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv3x3</span><span class="params">(in_planes,out_planes,stride=<span class="number">1</span>,dilation=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">''' 3x3 convolution with padding and dilation '''</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(in_planes,out_planes,kernel_size=<span class="number">3</span>,stride=stride,</span><br><span class="line">                    padding=dilation,dilation=dilation,bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv1x1</span><span class="params">(in_planes,out_planes,stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">''' 1x1 convolution '''</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(in_planes,out_planes,kernel_size=<span class="number">1</span>,stride=stride,bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,inplanes,planes,stride=<span class="number">1</span>,downsample=None,dilation=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(Bottleneck,self).__init__()</span><br><span class="line">        self.conv1 = conv1x1(inplanes,planes)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv2 = conv3x3(planes,planes,stride)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv3 = conv1x1(planes,planes*<span class="number">4</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes*<span class="number">4</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(x)</span><br><span class="line">        out = self.relu(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(x)</span><br><span class="line">        out = self.bn2(x)</span><br><span class="line">        out = self.relu(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(x)</span><br><span class="line">        out = self.bn3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,block,layers,num_classes=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(ResNet,self).__init__()</span><br><span class="line">        self.inplanes = <span class="number">64</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>,bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.layer1 = self._make_layer(block,<span class="number">64</span>,layers[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block,<span class="number">128</span>,layers[<span class="number">1</span>],stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block,<span class="number">256</span>,layers[<span class="number">2</span>],stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block,<span class="number">512</span>,layers[<span class="number">3</span>],stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># self.avgpool = nn.AdaptiveAvgPool2d((1,1))</span></span><br><span class="line">        <span class="comment"># self.fc = nn.Linear(512*expansion,num_classes)</span></span><br><span class="line"></span><br><span class="line">        <span class="string">''' init weight '''</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m,nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight,mode=<span class="string">'fan_out'</span>,nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias,<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m,nn.BatchNorm2d):</span><br><span class="line">                nn.init.constant_(m.weight,<span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self,block,planes,blocks,stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        downsample=<span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> stride !=<span class="number">1</span> <span class="keyword">or</span> self.inplanes !=planes*block.expansion:</span><br><span class="line">            downsample=nn.Sequential(</span><br><span class="line">                conv1x1(self.inplanes,planes*block.expansion),</span><br><span class="line">                nn.BatchNorm2d(planes*block.expansion),</span><br><span class="line">                )</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.inplanes,planes,stride,downsample))</span><br><span class="line">        self.inplanes = planes*block.expansion</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,blocks):</span><br><span class="line">            layers.append(block(self.inplanes,planes))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x_1 = self.layer1(x)</span><br><span class="line">        x_2 = self.layer2(x_1)</span><br><span class="line">        x_3 = self.layer3(x_2)</span><br><span class="line">        x_4 = self.layer4(x_3)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> x_3,x_4</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x_4</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet50</span><span class="params">(pretrained=True)</span>:</span></span><br><span class="line">    model = ResNet(Bottleneck,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="string">''' load the pretrained weight '''</span></span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        res_50=models.resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line">        new_dict = collections.OrderedDict()</span><br><span class="line">        <span class="keyword">for</span> (k1,v1),(k2,v2) <span class="keyword">in</span> zip(model.state_dict().items(),res_50.state_dict().items()):</span><br><span class="line">            new_dict[k1] = v2</span><br><span class="line">        model.load_state_dict(new_dict)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    resnet=resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line">    print(resnet)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt; PSPNET:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1612.01105&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Pyramid Scene Parsing Network&lt;/a&gt;&lt;/p&gt;
      
    
    </summary>
    
      <category term="Segmentation" scheme="http://www.hcrlp.com/categories/Segmentation/"/>
    
    
      <category term="Paper" scheme="http://www.hcrlp.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 参数初始化</title>
    <link href="http://www.hcrlp.com/2018/11/23/Pytorch-%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://www.hcrlp.com/2018/11/23/Pytorch-参数初始化/</id>
    <published>2018-11-23T02:13:51.000Z</published>
    <updated>2018-11-23T02:41:37.732Z</updated>
    
    <content type="html"><![CDATA[<p><strong> Pytorch 提供了很多不同的参数初始化函数：</strong></p><ul><li>torch.nn.init.constant_(tensor,val)</li><li>torch.nn.init.normal_(tensor,mean=0,std=1)</li><li>torch.nn.init.xavier_uniform_(tensor,gain=1)</li><li>更多的可以参考：<a href="http://pytorch.org/docs/nn.html#torch-nn-init" target="_blank" rel="noopener">http://pytorch.org/docs/nn.html#torch-nn-init</a></li></ul><p>注意上面的初始化函数的参数tensor，虽然写的是tensor，但是也可以是Variable类型的。而神经网络的参数类型Parameter是Variable类的子类，所以初始化函数可以直接作用于神经网络参数。</p><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>)</span><br><span class="line">init.xavier_uniform_(self.conv1.weight)</span><br><span class="line">init.constant_(self.conv1.bias, <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p><p>上面的语句是对网络的某一层参数进行初始化。如何对整个网络的参数进行初始化定制呢？<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier</span><span class="params">(parm)</span>:</span></span><br><span class="line">    init.xavier_normal_(parm)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname=m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        xavier(m.weight.data)</span><br><span class="line">        xavier(m.bias.data)</span><br><span class="line">net = Net()</span><br><span class="line">net.apply(weights_init) <span class="comment">#apply函数会递归地搜索网络内的所有module并把参数表示的函数应用到所有的module上</span></span><br></pre></td></tr></table></figure></p><p>不建议访问以下划线为前缀的成员，他们是内部的，如果有改变不会通知用户。更推荐的一种方法是检查某个module是否是某种类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier</span><span class="params">(parm)</span>:</span></span><br><span class="line">    init.xavier_normal_(parm)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">        xavier(m.weight.data)</span><br><span class="line">        xavier(m.bias.data)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt; Pytorch 提供了很多不同的参数初始化函数：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;torch.nn.init.constant_(tensor,val)&lt;/li&gt;
&lt;li&gt;torch.nn.init.normal_(tensor,mean=0,
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://www.hcrlp.com/categories/Deep-Learning/"/>
    
    
      <category term="Pytorch" scheme="http://www.hcrlp.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>CS231n学习笔记一：K近邻和图像分类</title>
    <link href="http://www.hcrlp.com/2018/11/08/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%9AK%E8%BF%91%E9%82%BB%E5%92%8C%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    <id>http://www.hcrlp.com/2018/11/08/CS231n学习笔记一：K近邻和图像分类/</id>
    <published>2018-11-08T10:29:59.000Z</published>
    <updated>2018-12-17T06:15:18.945Z</updated>
    
    <content type="html"><![CDATA[<h3 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h3><h4 id="1-概述："><a href="#1-概述：" class="headerlink" title="1. 概述："></a>1. 概述：</h4><p><strong>目标：</strong>所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。在后面的课程中，我们可以看到计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。</p><p><strong>例子：</strong>以下图为例，图像分类模型读取该图片，并生成该图片属于集合 {cat, dog, hat, mug}中各个标签的概率。需要注意的是，对于计算机来说，图像是一个由数字组成的巨大的3维数组。在这个例子中，猫的图像大小是宽248像素，高400像素，有3个颜色通道，分别是红、绿和蓝（简称RGB）。如此，该图像就包含了248X400X3=297600个数字，每个数字都是在范围0-255之间的整型，其中0表示全黑，255表示全白。我们的任务就是把这些上百万的数字变成一个简单的标签，比如“猫”。</p><p><img src="https://pic2.zhimg.com/80/baab9e4b97aceb77ec70abeda6be022d_hd.png" width="600"></p><h4 id="2-困难和挑战："><a href="#2-困难和挑战：" class="headerlink" title="2. 困难和挑战："></a>2. 困难和挑战：</h4><p>对于人来说，识别出一个像“猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。我们在下面列举了计算机视觉算法在图像识别方面遇到的一些困难，要记住图像是以3维数组来表示的，数组中的元素是亮度值。</p><ul><li>视角变化（Viewpoint variation）：同一个物体，摄像机可以从多个角度来展现。</li><li>大小变化（Scale variation）：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。</li><li>形变（Deformation）：很多东西的形状并非一成不变，会有很大变化。</li><li>遮挡（Occlusion）：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。</li><li>光照条件（Illumination conditions）：在像素层面上，光照的影响非常大。</li><li>背景干扰（Background clutter）：物体可能混入背景之中，使之难以被辨认。</li><li>类内差异（Intra-class variation）：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。<br>面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。</li></ul><p><img src="https://pic2.zhimg.com/80/1ee9457872f773d671dd5b225647ef45_hd.jpg" width="600"></p><h3 id="Nearest-Neighbor分类器"><a href="#Nearest-Neighbor分类器" class="headerlink" title="Nearest Neighbor分类器"></a>Nearest Neighbor分类器</h3><h4 id="L1-距离："><a href="#L1-距离：" class="headerlink" title="L1 距离："></a>L1 距离：</h4><p>针对CIFAR-10数据集，图片都是32x32x3的像素块，最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量I_1和I_2，然后计算他们的L1距离：</p><p><img src="http://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_1%28I_1%2CI_2%29%3D%5Csum_p%7CI%5Ep_1-I%5Ep_2%7C\"></p><p>可以通过下面的流程更清楚表示：</p><p><img src="https://pic2.zhimg.com/80/95cfe7d9efb83806299c218e0710a6c5_hd.jpg" width="600"></p><p>下面，让我们看看如何用代码来实现这个分类器。首先，我们将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，Xtr（大小是50000x32x32x3）存有训练集中所有的图像，Ytr是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class="string">'data/cifar10/'</span>) <span class="comment"># a magic function we provide</span></span><br><span class="line"><span class="comment"># flatten out all images to be one-dimensional</span></span><br><span class="line">Xtr_rows = Xtr.reshape(Xtr.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xtr_rows becomes 50000 x 3072</span></span><br><span class="line">Xte_rows = Xte.reshape(Xte.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xte_rows becomes 10000 x 3072</span></span><br></pre></td></tr></table></figure></p><p>现在我们得到所有的图像数据，并且把他们拉长成为行向量了。接下来展示如何训练并评价一个分类器：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nn = NearestNeighbor() <span class="comment"># create a Nearest Neighbor classifier class</span></span><br><span class="line">nn.train(Xtr_rows, Ytr) <span class="comment"># train the classifier on the training images and labels</span></span><br><span class="line">Yte_predict = nn.predict(Xte_rows) <span class="comment"># predict labels on the test images</span></span><br><span class="line"><span class="comment"># and now print the classification accuracy, which is the average number</span></span><br><span class="line"><span class="comment"># of examples that are correctly predicted (i.e. label matches)</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'accuracy: %f'</span> % ( np.mean(Yte_predict == Yte) )</span><br></pre></td></tr></table></figure></p><p>作为评价标准，我们常常使用准确率，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：train(X, y)函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个predict(X)函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用L1距离的Nearest Neighbor分类器的实现套路：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    <span class="string">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span></span><br><span class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">    self.Xtr = X</span><br><span class="line">    self.ytr = y</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">""" X is N x D where each row is an example we wish to predict label for """</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over all test rows</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># find the nearest training image to the i'th test image</span></span><br><span class="line">      <span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = <span class="number">1</span>)</span><br><span class="line">      min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">      Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure></p><h4 id="L2-距离："><a href="#L2-距离：" class="headerlink" title="L2 距离："></a>L2 距离：</h4><p>L2的距离公式为：  <img src="http://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_2%28I_1%2CI_2%29%3D%5Csqrt%7B+%5Csum_p%28I%5Ep_1-I%5Ep_2%29%5E2%7D">,所以我们在Numpy中，我们只需要替换上面代码中的1行代码就行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><h3 id="k-Nearest-Neighbor分类器"><a href="#k-Nearest-Neighbor分类器" class="headerlink" title="k-Nearest Neighbor分类器"></a>k-Nearest Neighbor分类器</h3><h4 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h4><p>相对于传统的 NN 方法，只找最相近的那1个图片的标签，我们也可以找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。</p><p><img src="https://pic3.zhimg.com/80/51aef845faa10195e33bdd4657592f86_hd.jpg" width="600"></p><p><strong><em>PS:</em></strong> 在NN分类器中，异常的数据点（比如：在蓝色区域中的绿点）制造出一个不正确预测的孤岛。5-NN分类器将这些不规则都平滑了，使得它针对测试数据的泛化（generalization）能力更好（例子中未展示）。注意，5-NN中也存在一些灰色区域，这些区域是因为近邻标签的最高票数相同导致的（比如：2个邻居是红色，2个邻居是蓝色，还有1个是绿色）。</p><h3 id="验证数据集"><a href="#验证数据集" class="headerlink" title="验证数据集"></a>验证数据集</h3><p>当你在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，绝不使用它。如果你使用测试集来调优，而且算法看起来效果不错，那么真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集过拟合。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能</p><blockquote><p>测试数据集只使用一次，即在训练完成后评价最终的模型时使用。</p></blockquote><p>思路是：从训练集中取出一部分数据用来调优，我们称之为验证集（validation set）。以CIFAR-10为例，我们可以用49000个图像作为训练集，用1000个图像作为验证集。验证集其实就是作为假的测试集来调优。下面就是代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span></span><br><span class="line"><span class="comment"># recall Xtr_rows is 50,000 x 3072 matrix</span></span><br><span class="line">Xval_rows = Xtr_rows[:<span class="number">1000</span>, :] <span class="comment"># take first 1000 for validation</span></span><br><span class="line">Yval = Ytr[:<span class="number">1000</span>]</span><br><span class="line">Xtr_rows = Xtr_rows[<span class="number">1000</span>:, :] <span class="comment"># keep last 49,000 for train</span></span><br><span class="line">Ytr = Ytr[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># find hyperparameters that work best on the validation set</span></span><br><span class="line">validation_accuracies = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># use a particular value of k and evaluation on validation data</span></span><br><span class="line">  nn = NearestNeighbor()</span><br><span class="line">  nn.train(Xtr_rows, Ytr)</span><br><span class="line">  <span class="comment"># here we assume a modified NearestNeighbor class that can take a k as input</span></span><br><span class="line">  Yval_predict = nn.predict(Xval_rows, k = k)</span><br><span class="line">  acc = np.mean(Yval_predict == Yval)</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'accuracy: %f'</span> % (acc,)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># keep track of what works on the validation set</span></span><br><span class="line">  validation_accuracies.append((k, acc))</span><br></pre></td></tr></table></figure></p><p>程序结束后，我们会作图分析出哪个k值表现最好，然后用这个k值来跑真正的测试集，并作出对算法的评价。</p><blockquote><p>把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。</p></blockquote><p>—————————————————————————————————————————<br><strong>交叉验证：</strong>有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为交叉验证的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。</p><p><img src="https://pic1.zhimg.com/80/6a3ceec60cc0a379b4939c37ee3e89e8_hd.png" width="600"><br>这就是5份交叉验证对k值调优的例子。针对每个k值，得到5个准确率结果，取其平均值，然后对不同k值的平均表现画线连接。本例中，当k=7的时算法表现最好（对应图中的准确率峰值）。如果我们将训练集分成更多份数，直线一般会更加平滑（噪音更少）。</p><p>—————————————————————————————————————————</p><p>实际应用。在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。</p><p><img src="https://pic1.zhimg.com/80/cc88207c6c3c5e91df8b6367368f6450_hd.jpg" width="600"></p><p>常用的数据分割模式。给出训练集和测试集后，训练集一般会被均分。这里是分成5份。前面4份用来训练，黄色那份用作验证集调优。如果采取交叉验证，那就各份轮流作为验证集。最后模型训练完毕，超参数都定好了，让模型跑一次（而且只跑一次）测试集，以此测试结果评价算法。</p><p>—————————————————————————————————————————</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><strong>简要说来：</strong></p><ol><li>介绍了图像分类问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。</li><li>介绍了一个简单的图像分类器：最近邻分类器(Nearest Neighbor classifier)。分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。</li><li>选取超参数的正确方法是：将原始训练集分为训练集和验证集，我们在验证集上尝试不同的超参数，最后保留表现最好那个。</li><li>如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音。</li><li>一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。</li><li>最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。</li><li>最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;图像分类&quot;&gt;&lt;a href=&quot;#图像分类&quot; class=&quot;headerlink&quot; title=&quot;图像分类&quot;&gt;&lt;/a&gt;图像分类&lt;/h3&gt;&lt;h4 id=&quot;1-概述：&quot;&gt;&lt;a href=&quot;#1-概述：&quot; class=&quot;headerlink&quot; title=&quot;1. 概述：&quot;
      
    
    </summary>
    
      <category term="Computer Vision" scheme="http://www.hcrlp.com/categories/Computer-Vision/"/>
    
    
      <category term="笔记" scheme="http://www.hcrlp.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习环境搭建</title>
    <link href="http://www.hcrlp.com/2018/10/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://www.hcrlp.com/2018/10/31/深度学习环境搭建/</id>
    <published>2018-10-31T02:56:23.000Z</published>
    <updated>2018-12-17T06:15:32.859Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DeepLearning-Environment"><a href="#DeepLearning-Environment" class="headerlink" title="DeepLearning-Environment"></a>DeepLearning-Environment</h1><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>Python 能够使用各种各样的开发环境，这里我们强烈推荐使用 Anaconda 来进行Python 环境的管理，当然如果你有自己偏好的 Python 环境管理方式，你完全可以使用自己更喜欢的方式。</p><p>1.登录 Anaconda 的官网 <a href="www.anaconda.com">www.anaconda.com</a>，选择下载</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fp8zfmxb5ej30lt0bb0tc.jpg" width="700"></p><p>2.选择对应的操作系统</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fp8zgyn21kj31i60qr771.jpg" width="600"></p><p>3.选择 Python 3.6 的版本进行下载，因为 Python 2.7 不久之后很多开源库都不再继续支持，所以我们的整个课程都是基于 Python 3.6 开发的，请务必选择正确的 Python 版本，Python 3.6</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fp8zkhinhzj31i60r3400.jpg" width="600"></p><p>4.下载完成进行安装即可</p><h3 id="Jupyter-安装和环境配置"><a href="#Jupyter-安装和环境配置" class="headerlink" title="Jupyter 安装和环境配置"></a>Jupyter 安装和环境配置</h3><p>安装完成之后，liunx/mac 打开终端，windows打开 power shell，输入<code>jupyter notebook</code>就可以在浏览器打开交互的 notebook 环境，可以在里面运行代码</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fp8zmwu2fuj31880pogmh.jpg" width="700"></p><h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>百度搜索 cuda，选择 CUDA Toolkit，进入 cuda 的官网，选择对应的操作系统进行下载</p><p>（注意 这里点进去直接是下载cuda9.1版本的，tensorflow 目前并不支持cuda9.1，我们可以从<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a>中找到适合的cuda版本，例如cuda9.0等等。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1foalzdh3j2j31bi0ur0yh.jpg" width="500"></p><p>进入之后和后面即将介绍的安装过程相同）</p><p>看到下面可以进行的系统选择</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1foalzkfgnjj31i60jg782.jpg" width="700"></p><p>对于 cuda 的安装，不同的操作系统有着不同的安装方式，这里仅以 linux 环境举例（这是配置亚马逊云环境中的一部分），关于windows 的配置可以动手百度或者google，对于 mac 电脑，12 年之后就不再使用nvidia 的GPU，所以没有办法安装cuda。</p><p>建议使用云服务器或者安装 linux 双系统，可以省去很多麻烦，也有助于后期深度学习的开发。</p><p>选择 linux 对应的 cuda 下载</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1foam0d72vpj31cm1ms7q0.jpg" width="500"></p><p>在终端输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://developer.nvidia.com/compute/cuda/9.1/Prod/local_installers/cuda_9.1.85_387.26_linux</span><br></pre></td></tr></table></figure><p>下载最新的 cuda 9，然后输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bash cuda_9.1.85_387.26_linux</span><br></pre></td></tr></table></figure><p>进行安装，接下来需要回答一些问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">accept/decline/quit: accept</span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 375.26?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line">Do you want to install the OpenGL libraries?</span><br><span class="line">(y)es/(n)o/(q)uit [ default is yes ]: y</span><br><span class="line">Do you want to run nvidia-xconfig?</span><br><span class="line">(y)es/(n)o/(q)uit [ default is no ]: n</span><br><span class="line">Install the CUDA 8.0 Toolkit?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line">Enter Toolkit Location</span><br><span class="line"> [ default is /usr/local/cuda-8.0 ]:</span><br><span class="line">Do you want to install a symbolic link at</span><br><span class="line">/usr/local/cuda?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line">Install the CUDA 8.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br></pre></td></tr></table></figure><p>运行完成之后就安装成功了，可以在终端输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p>查看GPU，最后我们需要将 cuda 添加在系统环境变量中方便以后的安装中找到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"export LD_LIBRARY_PATH=\$&#123;LD_LIBRARY_PATH&#125;:/usr/local/cuda-9.1/lib64"</span> &gt;&gt;~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="深度学习框架-TensorFlow-和-PyTorch-安装"><a href="#深度学习框架-TensorFlow-和-PyTorch-安装" class="headerlink" title="深度学习框架 TensorFlow 和 PyTorch 安装"></a>深度学习框架 TensorFlow 和 PyTorch 安装</h3><h4 id="TensorFlow-安装"><a href="#TensorFlow-安装" class="headerlink" title="TensorFlow 安装"></a>TensorFlow 安装</h4><p>目前 Tensorflow 支持在 Linux, MacOS, Windows 系统下安装，有仅支持 CPU 的版本，在缺少 GPU 资源时是一个不错的选择，也有 GPU 版本的实现高性能 GPU 加速。</p><p>在安装 GPU 版本之前需要一些额外的环境</p><h4 id="libcupti-dev"><a href="#libcupti-dev" class="headerlink" title="libcupti-dev"></a>libcupti-dev</h4><p>一行命令即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install libcupti-dev</span><br></pre></td></tr></table></figure><h4 id="cudnn"><a href="#cudnn" class="headerlink" title="cudnn"></a>cudnn</h4><p>进入 <a href="https://developer.nvidia.com/cudnn，点击下载" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn，点击下载</a></p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1foam0z0fykj319y0hxn7a.jpg" width="500"></p><p>会要求进行注册，点击 Join</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1foam18kpadj30ic09faa9.jpg" width="250"></p><p>然后填写关于你的一些信息就完成了注册。然后就可以打开 Download 出现下面的页面并选择下载压缩包</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1foam2cvftyj30qi0r5qaq.jpg" width="400"></p><p>解压后在当前目录运行下面命令即完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/include</span><br><span class="line">$ sudo cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda/lib64</span><br><span class="line">$ sudo chmod a+r /usr/<span class="built_in">local</span>/cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><h4 id="安装-Tensorflow"><a href="#安装-Tensorflow" class="headerlink" title="安装 Tensorflow"></a>安装 Tensorflow</h4><p>到这里 Tensorflow 的安装就非常简单了，可以在系统中用 pip 也可以在 anaconda 虚拟环境中安装</p><ul><li><p>pip 安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仅安装cpu版本 python2.x</span></span><br><span class="line">$ pip install tensorflow</span><br><span class="line"><span class="comment"># python3.x</span></span><br><span class="line">$ pip3 install tensorflow</span><br><span class="line"><span class="comment"># 安装gpu版本 python2.x</span></span><br><span class="line">$ pip install tensorflow-gpu</span><br><span class="line"><span class="comment"># python3.x</span></span><br><span class="line">$ pip3 install tensorflow-gpu</span><br></pre></td></tr></table></figure></li><li><p>anaconda安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 激活环境</span></span><br><span class="line"><span class="comment"># 下面的`$YOUR_ENV`替换成你自己的，没有的话要生成一个新的环境，可以参考下面注释的例子</span></span><br><span class="line"><span class="comment"># `conda create -n tensorflow pip python=2.7 # or python=3.3, etc.`</span></span><br><span class="line"><span class="comment"># 这样会构建一个名为 tensorflow，python 是2.7版本的虚拟环境</span></span><br><span class="line"><span class="comment"># 换名字很简单，换python版本的话也只需要将2.7改变即可，比如改变成3.6</span></span><br><span class="line">$ <span class="built_in">source</span> activate <span class="variable">$YOUR_ENV</span></span><br><span class="line"><span class="comment"># 在环境中安装tensorflow，注意这里的tfBinaryURL需要根据需求替换，后面详述</span></span><br><span class="line">(<span class="variable">$YOUR_ENV</span>)$ pip install --ignore-installed --upgrade tfBinaryURL</span><br></pre></td></tr></table></figure><p>tfBinaryURL 以在<a href="https://tensorflow.google.cn/install/install_linux#the_url_of_the_tensorflow_python_package选择" target="_blank" rel="noopener">https://tensorflow.google.cn/install/install_linux#the_url_of_the_tensorflow_python_package选择</a></p></li></ul><h4 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h4><p>终端中打开 python 解释器，运行下面命令成功即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(hello))</span><br></pre></td></tr></table></figure><h4 id="出现问题"><a href="#出现问题" class="headerlink" title="出现问题"></a>出现问题</h4><ul><li>更全面的 Tensorflow 安装页面 <a href="https://tensorflow.google.cn/install/" target="_blank" rel="noopener">https://tensorflow.google.cn/install/</a></li><li>检查硬件配置是否满足需求，GPU版本的 Tensorflow 需要计算能力在 3.5 及以上的显卡，可以在这里 <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a> 查到自己的显卡计算能力</li><li>在 Tensorflow 的 Github issues 里面寻找类似问题及解决方案</li></ul><h4 id="PyTorch-安装"><a href="#PyTorch-安装" class="headerlink" title="PyTorch 安装"></a>PyTorch 安装</h4><p>目前 PyTorch 官方只支持linux 和 MacOS，如果要查看 windows 的安装方法，请看后面。</p><p>在 linux 和 MacOS 这两个系统下进行安装非常的简单，访问到官网</p><p><a href="http://www.pytorch.org" target="_blank" rel="noopener">www.pytorch.org</a></p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1foam3kqe43j31i60ncn3m.jpg" width="700"></p><p>按照提示在终端输入命令行即可</p><h4 id="如何在-windows-下装-PyTorch"><a href="#如何在-windows-下装-PyTorch" class="headerlink" title="如何在 windows 下装 PyTorch"></a>如何在 windows 下装 PyTorch</h4><p>使用 windows 的同学可以访问这个链接查看如何在 windows 下面安装pytorch</p><p><a href="https://zhuanlan.zhihu.com/p/26871672" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26871672</a></p><h4 id="验证安装-1"><a href="#验证安装-1" class="headerlink" title="验证安装"></a>验证安装</h4><p>终端中打开 python 解释器，运行下面命令成功即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.Tensor([<span class="number">3</span>])</span><br><span class="line">print(x)</span><br><span class="line"><span class="comment"># x_gpu = torch.Tensor([3]).cuda() # GPU 安装验证</span></span><br><span class="line"><span class="comment"># print(x)</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;DeepLearning-Environment&quot;&gt;&lt;a href=&quot;#DeepLearning-Environment&quot; class=&quot;headerlink&quot; title=&quot;DeepLearning-Environment&quot;&gt;&lt;/a&gt;DeepLearning-E
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://www.hcrlp.com/categories/Deep-Learning/"/>
    
    
      <category term="Basic" scheme="http://www.hcrlp.com/tags/Basic/"/>
    
  </entry>
  
  <entry>
    <title>开篇之作</title>
    <link href="http://www.hcrlp.com/2018/10/30/%E5%BC%80%E7%AF%87%E4%B9%8B%E4%BD%9C/"/>
    <id>http://www.hcrlp.com/2018/10/30/开篇之作/</id>
    <published>2018-10-30T09:02:06.000Z</published>
    <updated>2018-10-30T09:21:43.826Z</updated>
    
    <content type="html"><![CDATA[<p>漫漫的两天时间终于搭建好了这个个人主页，nothing to say，非常开心！哈哈哈！</p><p>Keep Working, keep Moving!</p><p><em>加油</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;漫漫的两天时间终于搭建好了这个个人主页，nothing to say，非常开心！哈哈哈！&lt;/p&gt;
&lt;p&gt;Keep Working, keep Moving!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;加油&lt;/em&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="其他" scheme="http://www.hcrlp.com/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="随记" scheme="http://www.hcrlp.com/tags/%E9%9A%8F%E8%AE%B0/"/>
    
  </entry>
  
</feed>
