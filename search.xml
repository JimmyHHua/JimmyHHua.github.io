<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Neural Style Transfer]]></title>
    <url>%2F2018%2F12%2F29%2FNeural-Style-Transfer%2F</url>
    <content type="text"><![CDATA[IntroductionStyle Transfer 是一种通过CNN将一幅图像的内容与另一幅图像的风格相结合的算法.对于给定的一幅内容图像和一幅风格图像,算法的目标是生成一幅新的图像使得其与内容图像之间的内容差异最小,同时和风格图像之间的风格差异最小. 参考论文： A Neural Algorithm of Artistic Style Perceptual Losses for Real-Time Style Transferand Super-Resolution Principle利用卷积网络提取图像特征，构造出风格迁移的基本原理： 两张图像经过预训练好的分类网络，若提取出的高维特征(high−levelhigh−level)之间的欧氏距离越小，则这两张图像内容越相似 两张图像经过与训练好的分类网络，若提取出的低维特征(low−levellow−level)在数值上基本相等，则这两张图像越相似，换句话说，两张图像相似等价于二者特征的GramGram矩阵具有较小的弗罗贝尼乌斯范数。 Gram Matrix GramGram矩阵的数学形式如下： 1G(x) = A*A^T Gram矩阵实际上是矩阵的内积运算，在风格迁移算法中，其计算的是feature map之间的偏心协方差，在feature map 包含着图像的特征，每个数字表示特征的强度，Gram矩阵代表着特征之间的相关性，因此，Gram矩阵可以用来表示图像的风格，因此可以通过Gram矩阵衡量风格的差异性。 Content loss内容差异最小化–首先将内容图像和目标图像分别送入预训练模型VGGNet,以提取特征.然后,通过最小化两个特征图之间的均方误差mean-squared error来更新目标图像 1content_loss += torch.mean((target_features - content_features) ** 2) Style loss当计算content loss时,首先我们将风格图像和目标图像分别送入VGGNet来提取特征.为了生成纹理来匹配风格图像的风格,我们通过最小化协方差来更新目标图像.Style loss的计算方法可以看论文 1234567# f1,f2,f3 &gt; target_features, content_features, style_features# 计算gram矩阵f1 = torch.mm(f1, f1.t()) # f1乘以f1的转置f3 = torch.mm(f3, f3.t())# 利用目标图像和风格图像计算风格损失style_loss += torch.mean((f1 - f3) ** 2) / (c * h * w) VGGNet使用训练好的 vgg19 网络来提取特征图，选用 conv1_1 ~ conv5_1作为特征图。 12345678910111213141516class VGGNet(nn.Module): def __init__(self): """选择conv1_1 ~ conv5_1作为特征图.""" super(VGGNet, self).__init__() self.select = ['0', '5', '10', '19', '28'] self.vgg = models.vgg19(pretrained=True).features # vgg的到定义,可以作为pytorch如何实现finetune的参考(https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py) def forward(self, x): """提取特征图""" features = [] for name, layer in self.vgg._modules.items(): x = layer(x) if name in self.select: features.append(x) return features Load image导入图片，resize 到指定的大小，并且转化为torch 格式12345678910111213141516def load_image(image_path, transform=None, max_size=None, shape=None): """加载图像并将其转化为一个tensor.""" image = Image.open(image_path) if max_size: scale = max_size / max(image.size) size = np.array(image.size) * scale image = image.resize(size.astype(int), Image.ANTIALIAS) if shape: image = image.resize(shape, Image.LANCZOS) if transform: image = transform(image).unsqueeze(0) return image.to(device) Results风格转换： Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# -*- coding: utf-8 -*-from __future__ import divisionfrom torchvision import modelsfrom torchvision import transformsfrom PIL import Imageimport argparseimport torchimport torchvisionimport torch.nn as nnimport numpy as np# 设备设置device = torch.device("cuda" if torch.cuda.is_available() else "cpu")def load_image(image_path, transform=None, max_size=None, shape=None): """加载图像并将其转化为一个tensor.""" image = Image.open(image_path) if max_size: scale = max_size / max(image.size) size = np.array(image.size) * scale image = image.resize(size.astype(int), Image.ANTIALIAS) if shape: image = image.resize(shape, Image.LANCZOS) if transform: image = transform(image).unsqueeze(0) return image.to(device)class VGGNet(nn.Module): def __init__(self): """选择conv1_1 ~ conv5_1作为特征图.""" super(VGGNet, self).__init__() self.select = ['0', '5', '10', '19', '28'] self.vgg = models.vgg19(pretrained=True).features # vgg的到定义,可以作为pytorch如何实现finetune的参考(https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py) def forward(self, x): """提取特征图""" features = [] for name, layer in self.vgg._modules.items(): x = layer(x) if name in self.select: features.append(x) return featuresdef main(config): # 图像预处理 # VGGNet预训练模型是基于ImageNet完成,其中图像经过了mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]的归一化 # 在该项目中用相同的归一化操作 transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]) # 加载内容图像和风格图像 # 使两个图像的大小相同 content = load_image(config.content, transform, max_size=config.max_size) style = load_image(config.style, transform, shape=[content.size(2), content.size(3)]) # 将内容图像初始化为一幅目标图像 target = content.clone().requires_grad_(True) optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999]) vgg = VGGNet().to(device).eval() for step in range(config.total_step): # 提取特征图 target_features = vgg(target) content_features = vgg(content) style_features = vgg(style) style_loss = 0 content_loss = 0 for f1, f2, f3 in zip(target_features, content_features, style_features): # 利用目标图像和内容图像计算content loss content_loss += torch.mean((f1 - f2) ** 2) # reshape特征图 _, c, h, w = f1.size() f1 = f1.view(c, h * w) f3 = f3.view(c, h * w) # 计算gram矩阵 f1 = torch.mm(f1, f1.t()) # f1乘以f1的转置 f3 = torch.mm(f3, f3.t()) # 利用目标图像和风格图像计算风格损失 style_loss += torch.mean((f1 - f3) ** 2) / (c * h * w) # 计算总损失,反向传播即优化 loss = content_loss + config.style_weight * style_loss optimizer.zero_grad() loss.backward() optimizer.step() if (step + 1) % config.log_step == 0: print('Step [&#123;&#125;/&#123;&#125;], Content Loss: &#123;:.4f&#125;, Style Loss: &#123;:.4f&#125;' .format(step + 1, config.total_step, content_loss.item(), style_loss.item())) if (step + 1) % config.sample_step == 0: # 保存生成图像 denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44)) img = target.clone().squeeze() img = denorm(img).clamp_(0, 1) torchvision.utils.save_image(img, 'output-&#123;&#125;.png'.format(step + 1))if __name__ == "__main__": parser = argparse.ArgumentParser(description='neural style config') parser.add_argument('--content', type=str, default='png/lily.jpg') parser.add_argument('--style', type=str, default='png/style5.jpg') parser.add_argument('--max_size', type=int, default=400) parser.add_argument('--total_step', type=int, default=2000) parser.add_argument('--log_step', type=int, default=10) parser.add_argument('--sample_step', type=int, default=500) parser.add_argument('--style_weight', type=float, default=100) parser.add_argument('--lr', type=float, default=0.003) config = parser.parse_args() print(config) main(config)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spatial Transformer Network]]></title>
    <url>%2F2018%2F12%2F26%2FSpatial-Transformer-Network%2F</url>
    <content type="text"><![CDATA[AbstractSpatial transformer networks are a generalization of differentiable attention to any spatial transformation. Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model. For example, it can crop a region of interest, scale and correct the orientation of an image. It can be a useful mechanism because CNNs are not invariant to rotation and scale and more general affine transformations. STN has been learning how to deal with the images classification by amplifyinmg and eliminating background noise,and then get the standard input to improve the efficiency of the classification. You could get more imformation via the Cartoon NetworkSpatial transformer networks boils down to three main components : The localization network is a regular CNN which regresses the transformation parameters. The transformation is never learned explicitly from this dataset, instead the network learns automatically the spatial transformations that enhances the global accuracy. The grid generator generates a grid of coordinates in the input image corresponding to each pixel from the output image. The sampler uses the parameters of the transformation and applies it to the input image. Localisation NetworkThe localisation network take the input feature map (W,H,C), and retrun the outputs θ (spatial transformer parameters). We can get the parameters no matter fully connection or convolutional network. Grid GeneratorAs we got the θ parameters via the Localisation Net, and then using the affine transformation to transfer the pixel position from A to B. The affine formula is as below: The result of the affine transformation is as below: MNIST TransformationNow, we will try to work with the Mnist dataset to learn how to augment our network using a visual attention mechanism called spatial transformer networks. Data load123456789101112131415161718192021222324252627from __future__ import print_functionimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport torchvisionfrom torchvision import datasets, transformsimport matplotlib.pyplot as pltimport numpy as npplt.ion() # interactive modedevice = torch.device("cuda" if torch.cuda.is_availabe() else "cpu")#traning datatrain_loader = torch.utils.data.Dataloader( datasets.MNIST(root='.',train=True,download=True, transform=transforms.Compose([ transforms.ToTensor, transforms.Normalize((0.1307,),(0.3081,))])), batch_size = 64, shuffle=True,num_workers=4)# Test datasettest_loader = torch.utils.data.DataLoader( datasets.MNIST(root='.', train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True, num_workers=4) Network Architecture Note: We need the latest version of PyTorch that contains affine_grid and grid_sample modules. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) # Spatial transformer localization-network self.localization = nn.Sequential( nn.Conv2d(1, 8, kernel_size=7), nn.MaxPool2d(2, stride=2), nn.ReLU(True), nn.Conv2d(8, 10, kernel_size=5), nn.MaxPool2d(2, stride=2), nn.ReLU(True) ) # Regressor for the 3 * 2 affine matrix self.fc_loc = nn.Sequential( nn.Linear(10 * 3 * 3, 32), nn.ReLU(True), nn.Linear(32, 3 * 2) ) # Initialize the weights/bias with identity transformation self.fc_loc[2].weight.data.zero_() self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)) # Spatial transformer network forward function def stn(self, x): xs = self.localization(x) xs = xs.view(-1, 10 * 3 * 3) theta = self.fc_loc(xs) theta = theta.view(-1, 2, 3) grid = F.affine_grid(theta, x.size()) x = F.grid_sample(x, grid) return x def forward(self, x): # transform the input x = self.stn(x) # Perform the usual forward pass x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1)model = Net().to(device) Training Model1234567891011121314151617181920212223242526272829303132333435363738394041optimizer = optim.SGD(model.parameters(), lr=0.01)def train(epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % 500 == 0: print('Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))## A simple test procedure to measure STN the performances on MNIST.#def test(): with torch.no_grad(): model.eval() test_loss = 0 correct = 0 for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) # sum up batch loss test_loss += F.nll_loss(output, target, size_average=False).item() # get the index of the max log-probability pred = output.max(1, keepdim=True)[1] correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n' .format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Visualizing The STN Results123456789101112131415161718192021222324252627282930313233343536373839404142434445def convert_image_np(inp): """Convert a Tensor to numpy image.""" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) return inp# We want to visualize the output of the spatial transformers layer# after the training, we visualize a batch of input images and# the corresponding transformed batch using STN.def visualize_stn(): with torch.no_grad(): # Get a batch of training data data = next(iter(test_loader))[0].to(device) input_tensor = data.cpu() transformed_input_tensor = model.stn(data).cpu() in_grid = convert_image_np( torchvision.utils.make_grid(input_tensor)) out_grid = convert_image_np( torchvision.utils.make_grid(transformed_input_tensor)) # Plot the results side-by-side f, axarr = plt.subplots(1, 2) axarr[0].imshow(in_grid) axarr[0].set_title('Dataset Images') axarr[1].imshow(out_grid) axarr[1].set_title('Transformed Images')for epoch in range(1, 20 + 1): train(epoch) test()# Visualize the STN transformation on some input batchvisualize_stn()plt.ioff()plt.show() The results：]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch-迁移学习]]></title>
    <url>%2F2018%2F12%2F25%2FPytorch-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[IntroduceIn practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows: ConvNet as fixed feature extractor Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. Fine-tuning the ConvNet Instead of random initializaion, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual. Pretrained models SampleToday we use the transfer learning to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well. This dataset is a very small subset of imagenet. Data LoadDownload the data from here Import the tools123456789101112131415from __future__ import print_function, divisionimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.optim import lr_schedulerimport numpy as npimport torchvisionfrom torchvision import datasets, models, transformsimport matplotlib.pyplot as pltimport timeimport osimport copyplt.ion() # interactive mode Data Augment and Normalization for training1234567891011121314151617181920212223242526#just normalization for validationdata_transforms = &#123; 'train':transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.Normalize([0.485,0.456,0.406],[0.228,0.224,0.225]) ]), 'val':transforms.Compose([ transforms.Reszie(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.228,0.224,0.225]) ]),&#125;data_dir = 'data/hymenoptera_data'image_datasets = &#123;x:datasets.ImageFolder(os.path.join(data_dir,x),data_transforms[x]) for x in ['train','val']&#125;dataloders = &#123;x:torch.utils.data.Dataloader(image_datasets[x],bath_szie=4, shuffle=True,num_works=4) for x in ['train','val']&#125;dataset_sizes = &#123;x:len(image_datasets[x]) for x in ['train','val']&#125;class_names = image_datasets['train'].classesdevice = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") Visualize a few images1234567891011121314151617181920def imshow(inp, title=None): """Imshow for Tensor.""" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated# Get a batch of training datainputs, classes = next(iter(dataloaders['train']))# Make a grid from batchout = torchvision.utils.make_grid(inputs)imshow(out, title=[class_names[x] for x in classes]) Training ModelNow, let’s write a general function to train a model. Here, we will illustrate: Scheduling the learning rateSaving the best modelIn the following, parameter ==scheduler== is an LR scheduler object from ==torch.optim.lr_scheduler==. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch &#123;&#125;/&#123;&#125;'.format(epoch, num_epochs - 1)) print('-' * 10) # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train() #Set model to training mode else: model.eval() running_loss = 0.0 running_corrects = 0 for inputs, labels in dataloders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter optimizer.zero_grad() '''forward''' # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _,preds = torch.max(outputs,1) loss = criterion(outputs,labels) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() #statistics running_loss += loss.item()*inputs.size()[0] running_correct += torch.sum(preds == labels.data) epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print('&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'.format( phase, epoch_loss, epoch_acc)) # deep copy the model if phase == 'val' and epoch_acc &gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print('Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'.format( time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: &#123;:4f&#125;'.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model Visualizing the model predictions12345678910111213141516171819202122232425def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders['val']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis('off') ax.set_title('predicted: &#123;&#125;'.format(class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Finetuning the ConvNet12345678910111213model_ft = models.resnet18(pretrained=True)num_ftrs = model_ft.fc.in_featuresmodel_fc.fc = nn.Linear(num_ftrs,2)model_ft = model_ft.to(device)criterion = nn.CrossEntropyLoss()# Observe that all parameters are being optimizedoptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochs# torch.optim.lr_scheduler模块的StepLR类，表示每隔step_size个epoch就将学习率降为原来的gamma倍。exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Train and evaluateIt should take around 15-25 min on CPU. On GPU though, it takes less than a minute.1model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=25) 1visualize_model(model_ft) Convnet as fixed feature extractorHere, we need to freeze all the network except the final layer. We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward(). 123456789101112131415161718model_conv = torchvision.models.resnet18(pretrained=True)for param in model_conv.parameters(): param.requires_grad = False# Parameters of newly constructed modules have requires_grad=True by defaultnum_ftrs = model_conv.fc.in_featuresmodel_conv.fc = nn.Linear(num_ftrs, 2)model_conv = model_conv.to(device)criterion = nn.CrossEntropyLoss()# Observe that only parameters of final layer are being optimized as# opoosed to before.optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochsexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) train the model:12model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) show the image:1234visualize_model(model_conv)plt.ioff()plt.show()]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yolo2 训练自己数据集]]></title>
    <url>%2F2018%2F12%2F10%2FYolo2-%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[####1. 添加数据分类的各个类别名称文 data/rabbit.names: ####2. 添加配置文件/cfg/rabbit.data，并且修改自己需要的类别和训练数据集： ####3. 修改网络配置 /cfg/yolov2-tiny-voc.cfg: ####4. 修改 Makefile 改成GPU训练： ####5. 运行命令：12345* 训练：./darknet detector train cfg/rabbit.data cfg/yolov2-tiny.cfg darknet19_448.conv.23* 测试：./darknet detector test cfg/rabbit.data cfg/yolov2-tiny.cfg backup/tiny-yolo-1/yolov2-tiny_50000.weights data/pic_1350.png* recall：./darknet detector recall cfg/rabbit.data cfg/yolov2-tiny.cfg backup/tiny-yolo-1/yolov2-tiny_50000.weights]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>yolo2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语义分割-PSPNET]]></title>
    <url>%2F2018%2F11%2F29%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-PSPNET%2F</url>
    <content type="text"><![CDATA[PSPNET: Pyramid Scene Parsing Network From : CVPR 2017(IEEE Conference on Computer Vision and Pattern Recognition) 代码： PSPnet-Keras-TensorFlow PSPnet-Pytorch 效果： 对比传统方法： Abstract本文提出的金字塔池化模块( pyramid pooling module)能够聚合不同区域的上下文信息,从而提高获取全局信息的能力。实验表明这样的先验表示(即指代PSP这个结构)是有效的，在多个数据集上展现了优良的效果。 Introduction场景解析(Scene Parsing)的难度与场景的标签密切相关。先大多数先进的场景解析框架大多数基于FCN，但FCN存在的几个问题： Mismatched Relationship ：上下文关系匹配对理解复杂场景很重要，FCN缺少上下文推断能力 Confusion categories ：许多标签之间存在关联，可以通过标签之间的关系弥补 Inconspicuous Classes ：模型可能会忽略小的东西，而大的东西又可能超过FCN的接受范围 总结这些情况，许多问题出在FCN不能有效的处理场景之间的关系和全局信息。本论文提出了能够获取全局场景的深度网络PSPNet，能够融合合适的全局特征，将局部和全局信息融合到一起。并提出了一个适度监督损失的优化策略，在多个数据集上表现优异。 本文的主要贡献如下： 提出了一个金字塔场景解析网络，能够将难解析的场景信息特征嵌入基于FCN预测框架中 在基于深度监督损失ResNet上制定有效的优化策略 构建了一个实用的系统，用于场景解析和语义分割，并包含了实施细节 Related Work受到深度神经网络的驱动，场景解析和语义分割获得了极大的进展。例如FCN、ENet等工作。许多深度卷积神经网络为了扩大高层feature的感受野，常用dilated convolution(空洞卷积)、coarse-to-fine structure等方法。本文基于先前的工作，选择的baseline是带dilated network的FCN。 大多数语义分割模型的工作基于两个方面： 具有多尺度的特征融合，高层特征具有强的语义信息，底层特征包含更多的细节。 基于结构预测。例如使用CRF(条件随机场)做后端细化分割结果。 为了充分的利用全局特征层次先验知识来进行不同场景理解，本文提出的PSP模块能够聚合不同区域的上下文从而达到获取全局上下文的目的。 Architecture 1. Pyramid Pooling Module 前面也说到了，本文的一大贡献就是PSP模块。 在一般CNN中感受野可以粗略的认为是使用上下文信息的大小，论文指出在许多网络中没有充分的获取全局信息，所以效果不好。要解决这一问题，常用的方法是： 用全局平均池化处理。但这在某些数据集上，可能会失去空间关系并导致模糊。 由金字塔池化产生不同层次的特征最后被平滑的连接成一个FC层做分类。这样可以去除CNN固定大小的图像分类约束，减少不同区域之间的信息损失。 论文提出了一个具有层次全局优先级，包含不同子区域之间的不同尺度的信息，称之为pyramid pooling module。 该模块融合了4种不同金字塔尺度的特征，第一行红色是最粗糙的特征–全局池化生成单个bin输出，后面三行是不同尺度的池化特征。为了保证全局特征的权重，如果金字塔共有N个级别，则在每个级别后使用1×1的卷积将对于级别通道降为原本的1/N。再通过双线性插值获得未池化前的大小，最终concat到一起。 金字塔等级的池化核大小是可以设定的，这与送到金字塔的输入有关。论文中使用的4个等级，核大小分别为1×1，2×2，3×3，6×6。 示例代码：12345678910111213141516171819class PyramidPool(nn.Module): def __init__(self,in_planes,out_planes,sizes=(1,2,3,6)): super(PyramidPool,self).__init__() self.pool_list = [nn.ModuleList([self._make_pool(in_planes,size) for size in sizes])] self.bottleneck = nn.Conv2d((in_planes)*2,out_planes,kernel_size=1) self.relu = nn.ReLU def _make_pool(self,in_planes,size): return nn.Sequential( nn.AdaptiveAvgPool2d(output_size=(size,size)), nn.Conv2d(in_planes,in_planes//4,kernel_size=1,bias=False), ) def forward(self,x): priors = [F.upsample_bilinear(input=pool,size=x.size[2:]) for pool in self.pool_list]+[x] cat_priors = torch.cat(priors,1) bottle = self.bottleneck(cat_priors) return self.relu(bottle) 2. 整体框架 基础层经过预训练的模型(ResNet101)和空洞卷积策略提取feature map,提取后的feature map是输入的1/8大小 feature map经过Pyramid Pooling Module得到融合的带有整体信息的feature，在上采样与池化前的feature map相concat 最后过一个卷积层得到最终输出 3. Auxiliary Network 在ResNet101的基础上做了改进，除了使用后面的softmax分类做loss，额外的在第四阶段添加了一个辅助的loss，两个loss一起传播，使用不同的权重，共同优化参数。后续的实验证明这样做有利于快速收敛 PSPNet示例代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445class PSPNet(nn.Module): def __init__(self,num_class,sizes=(1,2,3,6),spp_size=2048,pretrained=True): super(PSPNet,self).__init__() self.resnet = resnet.resnet50(pretrained) self.spp = PyramidPool(spp_size,1024,sizes) #主分支 self.final = nn.Sequential( nn.Conv2d(4096,512,kernel_size=3,padding=1,dilation=1), nn.BatchNorm2d(512), nn.ReLU(), nn.Dropout2d(p=0.3), nn.Conv2d(512,num_class,1,1), ) # 辅助分支 self.aux = nn.Sequential( nn.Linear(1024,256), nn.ReLU(), nn.Linear(256,num_class), ) ''' init weight ''' for m in self.modules(): if isinstance(m,nn.Conv2d): nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias,0) elif isinstance(m,nn.BatchNorm2d): nn.init.constant_(m.weight,1) nn.init.constant_(m.bias,0) def forward(self,x): if self.training: aux,h = self.resnet(x) aux_ = self.aux(aux) else: h = self.resnet(x) h = spp(h) h = self.final(h) if self.training: return aux_,h else: return h Experiment论文在ImageNet scene parsing challenge 2016, PASCAL VOC 2012,Cityscapes 三个数据集上做了实验。 原作训练细节： 项目 设置 学习率 采用“poly”策略，即lr=lrbase∗(1−itermaxiter)power 设置lrbase=0.01,power=0.9，衰减动量设置为0.9 and 0.0001 迭代次数 ImageNet上设置150K,PASCAL VOC设置30K，Cityscapes设置90K 数据增强 随机翻转、尺寸在0.5到2之间缩放、角度在-10到10之间旋转、随机的高斯滤波 batchsize batch很重要，设置batch=16(这很吃显存啊~) 训练分支网络 设置辅助loss的权重为0.4 平台 Caffe Conclusion论文在结构上提供了一个pyramid pooling module，在不同层次上融合feature,达到语义和细节的融合。 模型的性能表现很大，但感觉主要归功于一个良好的特征提取层。在实验部分讲了很多训练细节，但还是很难复现，这里值得好好推敲一下。 AppendixResnet50 示例代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#!/usr/bin/env python# -*- coding: utf-8 -*-"""@Time: 2018-12-03 11:01:14@author: JimmyHua"""import torchimport collectionsfrom torch import nnimport torchvision.models as modelsdef conv3x3(in_planes,out_planes,stride=1,dilation=1): ''' 3x3 convolution with padding and dilation ''' return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=stride, padding=dilation,dilation=dilation,bias=False)def conv1x1(in_planes,out_planes,stride=1): ''' 1x1 convolution ''' return nn.Conv2d(in_planes,out_planes,kernel_size=1,stride=stride,bias=False)class Bottleneck(nn.Module): expansion = 4 def __init__(self,inplanes,planes,stride=1,downsample=None,dilation=1): super(Bottleneck,self).__init__() self.conv1 = conv1x1(inplanes,planes) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = conv3x3(planes,planes,stride) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = conv1x1(planes,planes*4) self.bn3 = nn.BatchNorm2d(planes*4) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self,x): residual = x out = self.conv1(x) out = self.bn1(x) out = self.relu(x) out = self.conv2(x) out = self.bn2(x) out = self.relu(x) out = self.conv3(x) out = self.bn3(x) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self,block,layers,num_classes=1000): super(ResNet,self).__init__() self.inplanes = 64 self.conv1 = nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1) self.layer1 = self._make_layer(block,64,layers[0]) self.layer2 = self._make_layer(block,128,layers[1],stride=2) self.layer3 = self._make_layer(block,256,layers[2],stride=2) self.layer4 = self._make_layer(block,512,layers[3],stride=2) # self.avgpool = nn.AdaptiveAvgPool2d((1,1)) # self.fc = nn.Linear(512*expansion,num_classes) ''' init weight ''' for m in self.modules(): if isinstance(m,nn.Conv2d): nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias,0) elif isinstance(m,nn.BatchNorm2d): nn.init.constant_(m.weight,1) nn.init.constant_(m.bias,0) def _make_layer(self,block,planes,blocks,stride=1): downsample=None if stride !=1 or self.inplanes !=planes*block.expansion: downsample=nn.Sequential( conv1x1(self.inplanes,planes*block.expansion), nn.BatchNorm2d(planes*block.expansion), ) layers = [] layers.append(block(self.inplanes,planes,stride,downsample)) self.inplanes = planes*block.expansion for i in range(1,blocks): layers.append(block(self.inplanes,planes)) return nn.Sequential(*layers) def forward(self,x): x = self.conv1(x) x = self.bn1(x) x = self.maxpool(x) x_1 = self.layer1(x) x_2 = self.layer2(x_1) x_3 = self.layer3(x_2) x_4 = self.layer4(x_3) if self.training: return x_3,x_4 else: return x_4def resnet50(pretrained=True): model = ResNet(Bottleneck,[3,4,6,3]) ''' load the pretrained weight ''' if pretrained: res_50=models.resnet50(pretrained=True) new_dict = collections.OrderedDict() for (k1,v1),(k2,v2) in zip(model.state_dict().items(),res_50.state_dict().items()): new_dict[k1] = v2 model.load_state_dict(new_dict) return modelif __name__ == '__main__': resnet=resnet50(pretrained=True) print(resnet)]]></content>
      <categories>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch 参数初始化]]></title>
    <url>%2F2018%2F11%2F23%2FPytorch-%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Pytorch 提供了很多不同的参数初始化函数： torch.nn.init.constant_(tensor,val) torch.nn.init.normal_(tensor,mean=0,std=1) torch.nn.init.xavier_uniform_(tensor,gain=1) 更多的可以参考：http://pytorch.org/docs/nn.html#torch-nn-init 注意上面的初始化函数的参数tensor，虽然写的是tensor，但是也可以是Variable类型的。而神经网络的参数类型Parameter是Variable类的子类，所以初始化函数可以直接作用于神经网络参数。 示例：123self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)init.xavier_uniform_(self.conv1.weight)init.constant_(self.conv1.bias, 0.1) 上面的语句是对网络的某一层参数进行初始化。如何对整个网络的参数进行初始化定制呢？12345678910def xavier(parm): init.xavier_normal_(parm)def weights_init(m): classname=m.__class__.__name__ if classname.find('Conv') != -1: xavier(m.weight.data) xavier(m.bias.data)net = Net()net.apply(weights_init) #apply函数会递归地搜索网络内的所有module并把参数表示的函数应用到所有的module上 不建议访问以下划线为前缀的成员，他们是内部的，如果有改变不会通知用户。更推荐的一种方法是检查某个module是否是某种类型： 1234567def xavier(parm): init.xavier_normal_(parm)def weights_init(m): if isinstance(m, nn.Conv2d): xavier(m.weight.data) xavier(m.bias.data)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记一：K近邻和图像分类]]></title>
    <url>%2F2018%2F11%2F08%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%9AK%E8%BF%91%E9%82%BB%E5%92%8C%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[图像分类1. 概述：目标：所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。在后面的课程中，我们可以看到计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。 例子：以下图为例，图像分类模型读取该图片，并生成该图片属于集合 {cat, dog, hat, mug}中各个标签的概率。需要注意的是，对于计算机来说，图像是一个由数字组成的巨大的3维数组。在这个例子中，猫的图像大小是宽248像素，高400像素，有3个颜色通道，分别是红、绿和蓝（简称RGB）。如此，该图像就包含了248X400X3=297600个数字，每个数字都是在范围0-255之间的整型，其中0表示全黑，255表示全白。我们的任务就是把这些上百万的数字变成一个简单的标签，比如“猫”。 2. 困难和挑战：对于人来说，识别出一个像“猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。我们在下面列举了计算机视觉算法在图像识别方面遇到的一些困难，要记住图像是以3维数组来表示的，数组中的元素是亮度值。 视角变化（Viewpoint variation）：同一个物体，摄像机可以从多个角度来展现。 大小变化（Scale variation）：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。 形变（Deformation）：很多东西的形状并非一成不变，会有很大变化。 遮挡（Occlusion）：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。 光照条件（Illumination conditions）：在像素层面上，光照的影响非常大。 背景干扰（Background clutter）：物体可能混入背景之中，使之难以被辨认。 类内差异（Intra-class variation）：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。 Nearest Neighbor分类器L1 距离：针对CIFAR-10数据集，图片都是32x32x3的像素块，最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量I_1和I_2，然后计算他们的L1距离： 可以通过下面的流程更清楚表示： 下面，让我们看看如何用代码来实现这个分类器。首先，我们将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，Xtr（大小是50000x32x32x3）存有训练集中所有的图像，Ytr是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072 现在我们得到所有的图像数据，并且把他们拉长成为行向量了。接下来展示如何训练并评价一个分类器：123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) ) 作为评价标准，我们常常使用准确率，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：train(X, y)函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个predict(X)函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用L1距离的Nearest Neighbor分类器的实现套路：123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred L2 距离：L2的距离公式为： ,所以我们在Numpy中，我们只需要替换上面代码中的1行代码就行：1distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) k-Nearest Neighbor分类器概念：相对于传统的 NN 方法，只找最相近的那1个图片的标签，我们也可以找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。 PS: 在NN分类器中，异常的数据点（比如：在蓝色区域中的绿点）制造出一个不正确预测的孤岛。5-NN分类器将这些不规则都平滑了，使得它针对测试数据的泛化（generalization）能力更好（例子中未展示）。注意，5-NN中也存在一些灰色区域，这些区域是因为近邻标签的最高票数相同导致的（比如：2个邻居是红色，2个邻居是蓝色，还有1个是绿色）。 验证数据集当你在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，绝不使用它。如果你使用测试集来调优，而且算法看起来效果不错，那么真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集过拟合。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能 测试数据集只使用一次，即在训练完成后评价最终的模型时使用。 思路是：从训练集中取出一部分数据用来调优，我们称之为验证集（validation set）。以CIFAR-10为例，我们可以用49000个图像作为训练集，用1000个图像作为验证集。验证集其实就是作为假的测试集来调优。下面就是代码：123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc)) 程序结束后，我们会作图分析出哪个k值表现最好，然后用这个k值来跑真正的测试集，并作出对算法的评价。 把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。 —————————————————————————————————————————交叉验证：有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为交叉验证的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。 这就是5份交叉验证对k值调优的例子。针对每个k值，得到5个准确率结果，取其平均值，然后对不同k值的平均表现画线连接。本例中，当k=7的时算法表现最好（对应图中的准确率峰值）。如果我们将训练集分成更多份数，直线一般会更加平滑（噪音更少）。 ————————————————————————————————————————— 实际应用。在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。 常用的数据分割模式。给出训练集和测试集后，训练集一般会被均分。这里是分成5份。前面4份用来训练，黄色那份用作验证集调优。如果采取交叉验证，那就各份轮流作为验证集。最后模型训练完毕，超参数都定好了，让模型跑一次（而且只跑一次）测试集，以此测试结果评价算法。 ————————————————————————————————————————— 小结简要说来： 介绍了图像分类问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。 介绍了一个简单的图像分类器：最近邻分类器(Nearest Neighbor classifier)。分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。 选取超参数的正确方法是：将原始训练集分为训练集和验证集，我们在验证集上尝试不同的超参数，最后保留表现最好那个。 如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音。 一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。 最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。 最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习环境搭建]]></title>
    <url>%2F2018%2F10%2F31%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[DeepLearning-EnvironmentPythonPython 能够使用各种各样的开发环境，这里我们强烈推荐使用 Anaconda 来进行Python 环境的管理，当然如果你有自己偏好的 Python 环境管理方式，你完全可以使用自己更喜欢的方式。 1.登录 Anaconda 的官网 www.anaconda.com，选择下载 2.选择对应的操作系统 3.选择 Python 3.6 的版本进行下载，因为 Python 2.7 不久之后很多开源库都不再继续支持，所以我们的整个课程都是基于 Python 3.6 开发的，请务必选择正确的 Python 版本，Python 3.6 4.下载完成进行安装即可 Jupyter 安装和环境配置安装完成之后，liunx/mac 打开终端，windows打开 power shell，输入jupyter notebook就可以在浏览器打开交互的 notebook 环境，可以在里面运行代码 CUDA百度搜索 cuda，选择 CUDA Toolkit，进入 cuda 的官网，选择对应的操作系统进行下载 （注意 这里点进去直接是下载cuda9.1版本的，tensorflow 目前并不支持cuda9.1，我们可以从https://developer.nvidia.com/cuda-toolkit-archive中找到适合的cuda版本，例如cuda9.0等等。 进入之后和后面即将介绍的安装过程相同） 看到下面可以进行的系统选择 对于 cuda 的安装，不同的操作系统有着不同的安装方式，这里仅以 linux 环境举例（这是配置亚马逊云环境中的一部分），关于windows 的配置可以动手百度或者google，对于 mac 电脑，12 年之后就不再使用nvidia 的GPU，所以没有办法安装cuda。 建议使用云服务器或者安装 linux 双系统，可以省去很多麻烦，也有助于后期深度学习的开发。 选择 linux 对应的 cuda 下载 在终端输入 1$ wget https://developer.nvidia.com/compute/cuda/9.1/Prod/local_installers/cuda_9.1.85_387.26_linux 下载最新的 cuda 9，然后输入 1$ bash cuda_9.1.85_387.26_linux 进行安装，接下来需要回答一些问题 12345678910111213141516accept/decline/quit: acceptInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 375.26?(y)es/(n)o/(q)uit: yDo you want to install the OpenGL libraries?(y)es/(n)o/(q)uit [ default is yes ]: yDo you want to run nvidia-xconfig?(y)es/(n)o/(q)uit [ default is no ]: nInstall the CUDA 8.0 Toolkit?(y)es/(n)o/(q)uit: yEnter Toolkit Location [ default is /usr/local/cuda-8.0 ]:Do you want to install a symbolic link at/usr/local/cuda?(y)es/(n)o/(q)uit: yInstall the CUDA 8.0 Samples?(y)es/(n)o/(q)uit: n 运行完成之后就安装成功了，可以在终端输入 1nvidia-smi 查看GPU，最后我们需要将 cuda 添加在系统环境变量中方便以后的安装中找到 12echo "export LD_LIBRARY_PATH=\$&#123;LD_LIBRARY_PATH&#125;:/usr/local/cuda-9.1/lib64" &gt;&gt;~/.bashrcsource ~/.bashrc 深度学习框架 TensorFlow 和 PyTorch 安装TensorFlow 安装目前 Tensorflow 支持在 Linux, MacOS, Windows 系统下安装，有仅支持 CPU 的版本，在缺少 GPU 资源时是一个不错的选择，也有 GPU 版本的实现高性能 GPU 加速。 在安装 GPU 版本之前需要一些额外的环境 libcupti-dev一行命令即可 1$ sudo apt-get install libcupti-dev cudnn进入 https://developer.nvidia.com/cudnn，点击下载 会要求进行注册，点击 Join 然后填写关于你的一些信息就完成了注册。然后就可以打开 Download 出现下面的页面并选择下载压缩包 解压后在当前目录运行下面命令即完成 123$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 安装 Tensorflow到这里 Tensorflow 的安装就非常简单了，可以在系统中用 pip 也可以在 anaconda 虚拟环境中安装 pip 安装 12345678# 仅安装cpu版本 python2.x$ pip install tensorflow# python3.x$ pip3 install tensorflow# 安装gpu版本 python2.x$ pip install tensorflow-gpu# python3.x$ pip3 install tensorflow-gpu anaconda安装 12345678# 激活环境# 下面的`$YOUR_ENV`替换成你自己的，没有的话要生成一个新的环境，可以参考下面注释的例子# `conda create -n tensorflow pip python=2.7 # or python=3.3, etc.`# 这样会构建一个名为 tensorflow，python 是2.7版本的虚拟环境# 换名字很简单，换python版本的话也只需要将2.7改变即可，比如改变成3.6$ source activate $YOUR_ENV# 在环境中安装tensorflow，注意这里的tfBinaryURL需要根据需求替换，后面详述($YOUR_ENV)$ pip install --ignore-installed --upgrade tfBinaryURL tfBinaryURL 以在https://tensorflow.google.cn/install/install_linux#the_url_of_the_tensorflow_python_package选择 验证安装终端中打开 python 解释器，运行下面命令成功即可 12345# Pythonimport tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello)) 出现问题 更全面的 Tensorflow 安装页面 https://tensorflow.google.cn/install/ 检查硬件配置是否满足需求，GPU版本的 Tensorflow 需要计算能力在 3.5 及以上的显卡，可以在这里 https://developer.nvidia.com/cuda-gpus 查到自己的显卡计算能力 在 Tensorflow 的 Github issues 里面寻找类似问题及解决方案 PyTorch 安装目前 PyTorch 官方只支持linux 和 MacOS，如果要查看 windows 的安装方法，请看后面。 在 linux 和 MacOS 这两个系统下进行安装非常的简单，访问到官网 www.pytorch.org 按照提示在终端输入命令行即可 如何在 windows 下装 PyTorch使用 windows 的同学可以访问这个链接查看如何在 windows 下面安装pytorch https://zhuanlan.zhihu.com/p/26871672 验证安装终端中打开 python 解释器，运行下面命令成功即可 123456# Pythonimport torchx = torch.Tensor([3])print(x)# x_gpu = torch.Tensor([3]).cuda() # GPU 安装验证# print(x)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Basic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开篇之作]]></title>
    <url>%2F2018%2F10%2F30%2F%E5%BC%80%E7%AF%87%E4%B9%8B%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[漫漫的两天时间终于搭建好了这个个人主页，nothing to say，非常开心！哈哈哈！ Keep Working, keep Moving! 加油]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>随记</tag>
      </tags>
  </entry>
</search>
